{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04ff097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-google-genai sentence-transformers chromadb PyPDF2 pandas tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import PyPDF2 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63cfdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b4caedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_PDF =  \"book.pdf\"\n",
    "QUERIES_JSON =  \"queries.json\"\n",
    "\n",
    "DOWNLOADS = Path.home() / \"Downloads\"\n",
    "OUTPUT_CSV = DOWNLOADS / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ad867ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 queries\n"
     ]
    }
   ],
   "source": [
    "# Load Queries\n",
    "with open(QUERIES_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# Normalize to list of dicts with {query_id, question}\n",
    "if isinstance(queries, dict):\n",
    "    if \"queries\" in queries:\n",
    "        queries = queries[\"queries\"]\n",
    "    else:\n",
    "        queries = [{\"query_id\": k, \"question\": v} for k, v in queries.items()]\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5026f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1 - Question: What is the scientific method in psychology?\n",
      "Query ID: 2 - Question: What are the basic parts of a neuron?\n",
      "Query ID: 3 - Question: What are the stages of sleep?\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Print first 5 queries\n",
    "for query in queries[:5]:\n",
    "    print(f\"Query ID: {query['query_id']} - Question: {query['question']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17feb656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 753 pages\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract PDF text\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    reader = PyPDF2.PdfReader(str(path))\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        pages.append({\"page\": i+1, \"text\": text})\n",
    "    return pages\n",
    "\n",
    "pages = extract_text_from_pdf(BOOK_PDF)\n",
    "print(f\"Extracted {len(pages)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5a1861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2201 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Chunk text\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for p in pages:\n",
    "    chunks = splitter.split_text(p[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        docs.append({\"text\": chunk, \"page\": p[\"page\"]})\n",
    "\n",
    "print(f\"Created {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f15a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma store created with 6603 documents\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Embeddings + Chroma\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = [d[\"text\"] for d in docs]\n",
    "metadatas = [{\"page\": d[\"page\"]} for d in docs]\n",
    "\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=\"chroma_store\"\n",
    ")\n",
    "print(\"Chroma store created with\", vectorstore._collection.count(), \"documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89e226d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: LLM (Gemma-3n-e2b-it)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemma-3n-e2b-it\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5d125fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prompt template\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an academic assistant. Using ONLY the provided context snippets (each has a page number),\n",
    "answer the user query precisely, concisely, and with references.\n",
    "\n",
    "If the answer is not present, reply \"Answer not found in the provided book.\"\n",
    "\n",
    "Return answer in 2 parts:\n",
    "1. The answer text\n",
    "2. A References section with page numbers mentioned\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ec125e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 3/3 [00:08<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Retrieval + Generation\n",
    "results = []\n",
    "for q in tqdm(queries, desc=\"Processing queries\"):\n",
    "    qid = q.get(\"query_id\")\n",
    "    question = q.get(\"question\")\n",
    "\n",
    "    # Retrieve top-5 chunks\n",
    "    retrieved = vectorstore.similarity_search(question, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        [f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\" for doc in retrieved]\n",
    "    )\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context_text, question=question)\n",
    "\n",
    "    # Call LLM\n",
    "    resp = llm([HumanMessage(content=prompt)])\n",
    "    answer_text = resp.content.strip()\n",
    "\n",
    "    # Build references JSON\n",
    "    pages = list({doc.metadata[\"page\"] for doc in retrieved})\n",
    "    references = {\"sections\": [], \"pages\": [str(p) for p in pages]}\n",
    "\n",
    "    results.append({\n",
    "        \"query_id\": qid,\n",
    "        \"context\": context_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"references\": json.dumps(references)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cf73bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: /Users/vaishnavipullakhandam/Downloads/submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Page 619]\\nVarious P sychother apy Techniques...</td>\n",
       "      <td>Answer not found in the provided book.\\n\\nRefe...</td>\n",
       "      <td>{\"sections\": [], \"pages\": [\"42\", \"619\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[Page 91]\\nthe other hand , ser ve as interc o...</td>\n",
       "      <td>1.  The neuron is the central building block o...</td>\n",
       "      <td>{\"sections\": [], \"pages\": [\"97\", \"91\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[Page 151]\\naspects o f sleep . Sev eral hormo...</td>\n",
       "      <td>The different stages of sleep are characterize...</td>\n",
       "      <td>{\"sections\": [], \"pages\": [\"150\", \"151\"]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                            context  \\\n",
       "0        1  [Page 619]\\nVarious P sychother apy Techniques...   \n",
       "1        2  [Page 91]\\nthe other hand , ser ve as interc o...   \n",
       "2        3  [Page 151]\\naspects o f sleep . Sev eral hormo...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Answer not found in the provided book.\\n\\nRefe...   \n",
       "1  1.  The neuron is the central building block o...   \n",
       "2  The different stages of sleep are characterize...   \n",
       "\n",
       "                                  references  \n",
       "0   {\"sections\": [], \"pages\": [\"42\", \"619\"]}  \n",
       "1    {\"sections\": [], \"pages\": [\"97\", \"91\"]}  \n",
       "2  {\"sections\": [], \"pages\": [\"150\", \"151\"]}  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Save submission.csv\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved submission to:\", OUTPUT_CSV)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
