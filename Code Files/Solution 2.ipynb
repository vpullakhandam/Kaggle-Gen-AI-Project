{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU \\\n",
    "  \"langchain>=0.3\" \\\n",
    "  \"langchain-community>=0.3\" \\\n",
    "  \"langchain-text-splitters>=0.3\" \\\n",
    "  \"langchain-chroma>=0.1.2\" \\\n",
    "  \"langchain-huggingface>=0.1.0\" \\\n",
    "  \"chromadb>=0.5\" \\\n",
    "  \"transformers>=4.44\" \\\n",
    "  \"sentence-transformers>=3.0\" \\\n",
    "  pdfplumber\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbec465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# New split packages (2025+)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.pdf import PDFPlumberLoader\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "\n",
    "# Models\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# ---------------- config you may edit ----------------\n",
    "PDF_PATH        = Path(\"../Sources/book.pdf\")          # <— point to your textbook PDF\n",
    "QUERIES_PATH    = Path(\"../Sources/queries.json\")          # expects [{\"id\": \"...\", \"question\": \"...\"}]\n",
    "SUBMISSION_CSV  = Path(\"./submission.csv\")\n",
    "\n",
    "PERSIST_DIR     = Path(\"../Code File/chroma_db\")             # Chroma on-disk store\n",
    "COLLECTION_NAME = \"psychology_textbook\"\n",
    "\n",
    "# RAG knobs (tuned for Llama-3.2 3B Instruct)\n",
    "CHUNK_SIZE      = 800     # ~200 tokens\n",
    "CHUNK_OVERLAP   = 200\n",
    "CANDIDATE_K     = 30\n",
    "FINAL_K         = 3\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "EMBED_MODEL     = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RERANK_MODEL    = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "LLM_MODEL       = \"microsoft/phi-2\"           # CPU-friendly, instruction-tuned\n",
    "\n",
    "SEED            = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319cf77",
   "metadata": {},
   "source": [
    "PDFPlumberLoader returns one doc per page with detailed metadata (incl. page index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabe23eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753,\n",
       " {'source': '../Sources/book.pdf',\n",
       "  'file_path': '../Sources/book.pdf',\n",
       "  'page': 1,\n",
       "  'total_pages': 753,\n",
       "  'CreationDate': \"D:20220224092550-06'00'\",\n",
       "  'ModDate': \"D:20220301111804-06'00'\",\n",
       "  'Producer': 'Prince 14.2 (www.princexml.com)',\n",
       "  'Title': 'Psychology 2e'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert PDF_PATH.exists(), f\"PDF not found at {PDF_PATH.resolve()}\"\n",
    "\n",
    "loader = PDFPlumberLoader(str(PDF_PATH))\n",
    "pages: list[Document] = loader.load()  # one Document per page; includes page metadata\n",
    "len(pages), pages[1].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee0dba",
   "metadata": {},
   "source": [
    "split into chunks (preserve page metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b10052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3794,\n",
       " {'source': '../Sources/book.pdf',\n",
       "  'file_path': '../Sources/book.pdf',\n",
       "  'page': 2,\n",
       "  'total_pages': 753,\n",
       "  'CreationDate': \"D:20220224092550-06'00'\",\n",
       "  'ModDate': \"D:20220301111804-06'00'\",\n",
       "  'Producer': 'Prince 14.2 (www.princexml.com)',\n",
       "  'Title': 'Psychology 2e'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursive splitter keeps larger structures intact where possible.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # fallback cascade\n",
    ")\n",
    "\n",
    "chunks: list[Document] = splitter.split_documents(pages)\n",
    "\n",
    "# carry forward page numbers for citations\n",
    "for d in chunks:\n",
    "    # Ensure we retain the original page id if present (pdfplumber sets 'page' or 'page_number' keys)\n",
    "    pg = d.metadata.get(\"page\", d.metadata.get(\"page_number\"))\n",
    "    if pg is not None:\n",
    "        d.metadata[\"page\"] = int(pg)\n",
    "\n",
    "len(chunks), chunks[0].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df482381",
   "metadata": {},
   "source": [
    "embeddings & Chroma index (persisted)\n",
    "\n",
    "Use HuggingFaceEmbeddings from the partner package; Chroma’s current integration lives in langchain-chroma, supports local persistence. Normalizing embeddings is a common practice for cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2abc33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma collection: psychology_textbook size: 2882\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": DEVICE},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # good for cosine similarity\n",
    ")\n",
    "\n",
    "# Fresh or existing DB\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "\n",
    "# Add only if empty to avoid duplicates when re-running\n",
    "if vectorstore._collection.count() == 0:\n",
    "    vectorstore.add_documents(chunks)\n",
    "    vectorstore.persist()\n",
    "\n",
    "print(\"Chroma collection:\", COLLECTION_NAME, \"size:\", vectorstore._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba37aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast recall → semantic retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": CANDIDATE_K})\n",
    "\n",
    "# Precise ranking → cross-encoder\n",
    "reranker = CrossEncoder(RERANK_MODEL, device=DEVICE)\n",
    "\n",
    "def cross_encoder_rerank(query: str, docs: list[Document], top_k: int = FINAL_K) -> list[Document]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = reranker.predict(pairs)  # higher = more relevant\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: float(x[1]), reverse=True)\n",
    "    return [d for d, _ in ranked[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55453af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(query: str) -> tuple[list[Document], str, list[int]]:\n",
    "    # 1) retrieve candidates\n",
    "    candidates = retriever.invoke(query)\n",
    "    # 2) rerank with cross-encoder\n",
    "    top_docs = cross_encoder_rerank(query, candidates, top_k=FINAL_K)\n",
    "    # 3) build context string (keep original order from reranker)\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content.strip() for d in top_docs)\n",
    "    # 4) collect unique page numbers for citations\n",
    "    pages = sorted({int(d.metadata.get(\"page\")) for d in top_docs if d.metadata.get(\"page\") is not None})\n",
    "    return top_docs, context, pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bac7546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.74s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 384.00 KiB, max allowed: 9.07 GiB). Tried to allocate 25.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load tokenizer + model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(LLM_MODEL)\n\u001b[0;32m----> 8\u001b[0m model     \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Build generation pipeline\u001b[39;00m\n\u001b[1;32m     11\u001b[0m gen_pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m   \u001b[38;5;66;03m# ensures we only get the generated part\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:4459\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4458\u001b[0m         )\n\u001b[0;32m-> 4459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1369\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 928 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:955\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 955\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1350\u001b[0m             device,\n\u001b[1;32m   1351\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1352\u001b[0m             non_blocking,\n\u001b[1;32m   1353\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1354\u001b[0m         )\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 384.00 KiB, max allowed: 9.07 GiB). Tried to allocate 25.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model     = AutoModelForCausalLM.from_pretrained(LLM_MODEL).to(DEVICE)\n",
    "\n",
    "# Build generation pipeline\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if DEVICE != \"cpu\" else -1,\n",
    "    max_new_tokens=280,      # ~200 words\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.15,\n",
    "    return_full_text=False   # ensures we only get the generated part\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
    "\n",
    "print(\"✅ Gemma-2B text-generation pipeline ready on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question in an academic style using only the provided context.\n",
    "Write at least 200 words, structured with introduction, explanation, and conclusion.\n",
    "Do not copy text verbatim; paraphrase and expand ideas clearly.\n",
    "Cite textbook pages inline like [p. <number>] when relevant.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str) -> dict:\n",
    "    docs, context, pages = prepare_context(question)\n",
    "    prompt_str = prompt.format(question=question, context=context)\n",
    "\n",
    "    outputs = gen_pipe(prompt_str)\n",
    "\n",
    "    # Gemma returns \"generated_text\"\n",
    "    answer = outputs[0][\"generated_text\"].strip()\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"context\": context,\n",
    "        \"references\": {\"pages\": pages}\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "test_out = answer_question(\"What is the scientific method in psychology?\")\n",
    "test_out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
