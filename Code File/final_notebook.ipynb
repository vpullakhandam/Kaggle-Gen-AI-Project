{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "327d897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain_community chromadb pymupdf PyPDF2 FlagEmbedding\n",
    "%pip install -q sentence-transformers transformers torch pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2737883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Document processing\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Embeddings and vector store\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Reranking\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "# LLM\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Data validation\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Prevent tokenizer warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a495d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BOOK_PDF_PATH = Path(\"../Sources/book.pdf\")\n",
    "SECTIONS_JSON_PATH = Path(\"./working/page_to_section.json\")\n",
    "CHROMA_PATH = Path(\"./chroma\")\n",
    "\n",
    "# Model settings\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "RERANKER_MODEL = \"BAAI/bge-reranker-v2-m3\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Document processing settings\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "LOGICAL_PAGE_OFFSET = 12  # Difference between physical and logical page numbers\n",
    "\n",
    "# Verify paths exist\n",
    "assert BOOK_PDF_PATH.exists(), f\"PDF not found: {BOOK_PDF_PATH}\"\n",
    "assert SECTIONS_JSON_PATH.exists(), f\"Sections JSON not found: {SECTIONS_JSON_PATH}\"\n",
    "CHROMA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Device being used:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f93e127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sections mapping...\n",
      "Loaded 494 page-to-section mappings\n"
     ]
    }
   ],
   "source": [
    "# Load section mapping\n",
    "def load_section_mapping():\n",
    "    \"\"\"Load and prepare section mapping\"\"\"\n",
    "    print(\"Loading sections mapping...\")\n",
    "    with open(SECTIONS_JSON_PATH, \"r\") as f:\n",
    "        sections_map = json.load(f)\n",
    "    \n",
    "    # Ensure all keys are strings\n",
    "    sections_map = {str(k): v for k, v in sections_map.items()}\n",
    "    print(f\"Loaded {len(sections_map)} page-to-section mappings\")\n",
    "    return sections_map\n",
    "\n",
    "# Text cleaning\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text, removing supplementary sections\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove supplementary sections\n",
    "    sections_to_remove = [\n",
    "        r'Key Terms\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Summary\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Review Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Critical Thinking Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Personal Application Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Access for free at openstax\\.org\\.*',\n",
    "        r'LINK TO LEARNING.*?(?=\\n|$)',\n",
    "        r'Watch a brief video.*?(?=\\n|$)',\n",
    "        r'http[s]?://\\S+'\n",
    "    ]\n",
    "    \n",
    "    for pattern in sections_to_remove:\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Load sections mapping\n",
    "sections_map = load_section_mapping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b580acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for validation\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a single chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    physical_page: int\n",
    "    logical_page: int\n",
    "    section: str\n",
    "\n",
    "class RetrievedChunk(BaseModel):\n",
    "    \"\"\"A single retrieved chunk with metadata\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "    score: float\n",
    "\n",
    "class QueryResults(BaseModel):\n",
    "    \"\"\"Results for a single query\"\"\"\n",
    "    query: str\n",
    "    chunks: List[RetrievedChunk] = Field(\n",
    "        ...,\n",
    "        min_items=5,\n",
    "        max_items=5,\n",
    "        description=\"Top 5 retrieved chunks for this query\"\n",
    "    )\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"Final RAG output\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    source_chunks: List[ChunkMetadata]\n",
    "    sections_referenced: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fca99903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for validation\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a single chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    physical_page: int\n",
    "    logical_page: int\n",
    "    section: str\n",
    "\n",
    "class RetrievedChunk(BaseModel):\n",
    "    \"\"\"A single retrieved chunk with metadata\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "    score: float\n",
    "\n",
    "class QueryVariation(BaseModel):\n",
    "    \"\"\"A single query variation with its results\"\"\"\n",
    "    query: str\n",
    "    chunks: List[RetrievedChunk] = Field(\n",
    "        ...,\n",
    "        min_items=5,\n",
    "        max_items=5,\n",
    "        description=\"Top 5 retrieved chunks for this query\"\n",
    "    )\n",
    "\n",
    "class QueryExpansionResult(BaseModel):\n",
    "    \"\"\"Results from query expansion\"\"\"\n",
    "    original_query: str\n",
    "    variations: List[QueryVariation] = Field(\n",
    "        ...,\n",
    "        min_items=5,\n",
    "        max_items=5,\n",
    "        description=\"Original query plus 4 variations\"\n",
    "    )\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"Final RAG output\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    source_chunks: List[ChunkMetadata]\n",
    "    sections_referenced: List[str]\n",
    "    query_variations: List[str]  # Store the variations used\n",
    "\n",
    "    @field_validator(\"source_chunks\")\n",
    "    def validate_chunks(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError(\"Must have at least one source chunk\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"sections_referenced\")\n",
    "    def validate_sections(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError(\"Must have at least one section referenced\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"query_variations\")\n",
    "    def validate_variations(cls, v):\n",
    "        if len(v) != 5:\n",
    "            raise ValueError(\"Must have exactly 5 query variations\")\n",
    "        if len(set(v)) != len(v):\n",
    "            raise ValueError(\"All query variations must be unique\")\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04b8a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Flan-T5-small model...\n",
      "LLM setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup LLM for generation\n",
    "print(\"Loading Flan-T5-small model...\")\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "def generate_response(query: str, context: str) -> str:\n",
    "    \"\"\"Generate response using T5 model\"\"\"\n",
    "    prompt = f\"\"\"Based on the provided context, answer the question clearly and accurately.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Instructions:\n",
    "    1. Use ONLY information from the context\n",
    "    2. Be specific and accurate\n",
    "    3. Include relevant page references [Page X] when possible\n",
    "    4. Provide a clear, structured explanation\n",
    "    5. If context doesn't contain enough information, say so\n",
    "\n",
    "    Format:\n",
    "    - Start with a clear definition or main point\n",
    "    - Provide supporting details from the context\n",
    "    - Include page references in [brackets]\n",
    "    - End with a brief summary if appropriate\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=300,  # Increased for more complete answers\n",
    "        min_length=100,  # Increased for more detailed responses\n",
    "        num_beams=5,     # More beam search paths\n",
    "        length_penalty=1.5,  # Favor longer responses\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_query_variations(query: str) -> List[str]:\n",
    "    \"\"\"Generate query variations using LLM\"\"\"\n",
    "    # Extract core concept from query\n",
    "    core_concept = query.lower().replace('what is ', '').replace('?', '').strip()\n",
    "    \n",
    "    # Template-based variations that work well for textbook content\n",
    "    variations = [\n",
    "        query,  # Original query\n",
    "        f\"How would you define {core_concept} in academic terms?\",\n",
    "        f\"What are the main concepts and principles of {core_concept}?\",\n",
    "        f\"Can you explain the fundamental aspects of {core_concept}?\",\n",
    "        f\"What are the key characteristics that define {core_concept}?\"\n",
    "    ]\n",
    "    \n",
    "    return variations\n",
    "    \n",
    "    # Generate variations\n",
    "    response = generate_response(query=query, context=prompt)\n",
    "    \n",
    "    # Split into lines and clean\n",
    "    variations = [\n",
    "        line.strip().split('.', 1)[-1].strip()  # Remove numbering\n",
    "        for line in response.split('\\n')\n",
    "        if line.strip() and any(c.isdigit() for c in line)  # Only lines with numbers\n",
    "    ]\n",
    "    \n",
    "    # Ensure variations are unique\n",
    "    variations = list(dict.fromkeys(variations))  # Remove duplicates\n",
    "    \n",
    "    # Add original query at the start\n",
    "    all_queries = [query]\n",
    "    \n",
    "    # Add unique variations\n",
    "    for var in variations:\n",
    "        if var not in all_queries and len(all_queries) < 5:\n",
    "            all_queries.append(var)\n",
    "    \n",
    "    # If we still need more variations, add template-based ones\n",
    "    template_variations = [\n",
    "        f\"Explain the concept of {query.lower().replace('what is ', '')}\",\n",
    "        f\"What are the key aspects of {query.lower().replace('what is ', '')}\",\n",
    "        f\"How would you define {query.lower().replace('what is ', '')}\",\n",
    "        f\"What does {query.lower().replace('what is ', '')} encompass\"\n",
    "    ]\n",
    "    \n",
    "    for var in template_variations:\n",
    "        if len(all_queries) >= 5:\n",
    "            break\n",
    "        if var not in all_queries:\n",
    "            all_queries.append(var)\n",
    "    \n",
    "    return all_queries[:5]  # Return exactly 5 unique queries\n",
    "\n",
    "def generate_answer(query: str, context: List[Document]) -> str:\n",
    "    \"\"\"Generate final answer using LLM\"\"\"\n",
    "    # Format context\n",
    "    formatted_context = \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata['physical_page']} - {doc.metadata['section']}]:\\n{doc.page_content}\"\n",
    "        for doc in context\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the provided context, answer the question clearly and accurately.\n",
    "    \n",
    "    Context:\n",
    "    {formatted_context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Use only information from the provided context\n",
    "    2. Be specific and accurate\n",
    "    3. Include relevant page references [Page X] in your answer\n",
    "    4. If context doesn't contain enough information, say so\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    return generate_response(query=query, context=prompt)\n",
    "\n",
    "print(\"LLM setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1aa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query generation and retrieval\n",
    "def generate_query_variations(query: str) -> List[str]:\n",
    "    \"\"\"Generate academically-focused query variations\"\"\"\n",
    "    # Extract core concept from query\n",
    "    core_concept = query.lower().replace('what is ', '').replace('?', '').strip()\n",
    "    \n",
    "    # Academic-focused variations\n",
    "    variations = [\n",
    "        query,  # Original query\n",
    "        f\"Define {core_concept} as a field of study\",\n",
    "        f\"What are the fundamental principles of {core_concept}?\",\n",
    "        f\"How is {core_concept} defined in academic terms?\",\n",
    "        f\"What are the core concepts and methods of {core_concept}?\"\n",
    "    ]\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def reciprocal_rank_fusion(all_results: List[tuple], k: int = 60) -> List[tuple]:\n",
    "    \"\"\"Combine results using Reciprocal Rank Fusion\"\"\"\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    # Calculate RRF scores\n",
    "    for rank, (doc, score) in enumerate(all_results):\n",
    "        doc_id = doc.metadata['chunk_id']\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1/(rank + k)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    sorted_docs = sorted(\n",
    "        [(doc, score) for doc, score in all_results],\n",
    "        key=lambda x: rrf_scores[x[0].metadata['chunk_id']],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Take top 3 unique chunks\n",
    "    seen_ids = set()\n",
    "    final_results = []\n",
    "    for doc, score in sorted_docs:\n",
    "        if doc.metadata['chunk_id'] not in seen_ids:\n",
    "            seen_ids.add(doc.metadata['chunk_id'])\n",
    "            final_results.append((doc, score))\n",
    "            if len(final_results) == 3:\n",
    "                break\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def retrieve_and_rerank(query: str, vectorstore, reranker, top_k: int = 5) -> List[tuple]:\n",
    "    \"\"\"Retrieve and rerank chunks for a single query\"\"\"\n",
    "    # Get initial results with more candidates for reranking\n",
    "    results = vectorstore.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k * 2  # Get more candidates for better reranking\n",
    "    )\n",
    "    \n",
    "    # Rerank\n",
    "    pairs = [[query, doc.page_content] for doc, _ in results]\n",
    "    rerank_scores = reranker.compute_score(pairs)\n",
    "    \n",
    "    # Sort by reranker score\n",
    "    scored_results = [(doc, float(score)) for (doc, _), score in zip(results, rerank_scores)]\n",
    "    scored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_results[:top_k]\n",
    "\n",
    "# Example usage\n",
    "test_query = \"What is psychology?\"\n",
    "variations = generate_query_variations(test_query)\n",
    "print(\"Original:\", test_query)\n",
    "print(\"\\nGenerated variations:\")\n",
    "for i, var in enumerate(variations[1:], 1):\n",
    "    print(f\"{i}. {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG pipeline\n",
    "def process_query(query: str) -> FinalResponse:\n",
    "    \"\"\"Process a query through the full RAG pipeline\"\"\"\n",
    "    # 1. Generate query variations\n",
    "    variations = generate_query_variations(query)\n",
    "    print(f\"Generated {len(variations)} query variations:\")\n",
    "    for i, var in enumerate(variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    \n",
    "    # 2. Get results for each variation\n",
    "    all_results = []\n",
    "    for var in variations:\n",
    "        # Get results for this variation\n",
    "        results = retrieve_and_rerank(var, vectorstore, reranker)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 3. Apply RRF to get final chunks\n",
    "    final_chunks = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    # 4. Extract unique sections\n",
    "    sections = list(set(\n",
    "        doc.metadata['section'] \n",
    "        for doc, _ in final_chunks\n",
    "    ))\n",
    "    \n",
    "    # 5. Generate answer using LLM\n",
    "    final_docs = [doc for doc, _ in final_chunks]\n",
    "    answer = generate_answer(query, final_docs)\n",
    "    \n",
    "    # 6. Create final response\n",
    "    response = FinalResponse(\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        source_chunks=[\n",
    "            ChunkMetadata(\n",
    "                chunk_id=doc.metadata['chunk_id'],\n",
    "                physical_page=doc.metadata['physical_page'],\n",
    "                logical_page=doc.metadata['logical_page'],\n",
    "                section=doc.metadata['section']\n",
    "            )\n",
    "            for doc, _ in final_chunks\n",
    "        ],\n",
    "        sections_referenced=sections,\n",
    "        query_variations=variations\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the pipeline\n",
    "def test_pipeline():\n",
    "    \"\"\"Test the RAG pipeline with a sample query\"\"\"\n",
    "    query = \"What is psychology?\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    response = process_query(query)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Query: {response.query}\")\n",
    "    print(\"\\nQuery variations used:\")\n",
    "    for i, var in enumerate(response.query_variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(\"\\nSource chunks:\")\n",
    "    for chunk in response.source_chunks:\n",
    "        print(f\"- Page {chunk.physical_page} (Section: {chunk.section})\")\n",
    "    print(\"\\nSections referenced:\")\n",
    "    for section in response.sections_referenced:\n",
    "        print(f\"- {section}\")\n",
    "\n",
    "# Run test\n",
    "test_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f349f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is psychology?\n",
      "Generated 5 query variations:\n",
      "0. What is psychology?\n",
      "1. How would you define psychology in academic terms?\n",
      "2. What are the main concepts and principles of psychology?\n",
      "3. Can you explain the fundamental aspects of psychology?\n",
      "4. What are the key characteristics that define psychology?\n",
      "\n",
      "Results:\n",
      "Query: What is psychology?\n",
      "\n",
      "Query variations used:\n",
      "0. What is psychology?\n",
      "1. How would you define psychology in academic terms?\n",
      "2. What are the main concepts and principles of psychology?\n",
      "3. Can you explain the fundamental aspects of psychology?\n",
      "4. What are the key characteristics that define psychology?\n",
      "\n",
      "Answer: Cognitive psychology is a relatively young science with its experimental roots in the 19th century, compared, for example, to human physiology, which dates much earlier. As mentioned, anyone interested in [Page 33 - introduction_to_psychology/contemporary_Psychology]: revolution created an impetus for psychologists to focus their attention on better understanding the mind and mental processes that underlie behavior. Thus, cognitive psychology is the area of psychology that focuses on studying cognitions, or thoughts, and their relationship to our experiences and our actions.\n",
      "\n",
      "Source chunks:\n",
      "- Page 20 (Section: introduction_to_psychology/what_is_psychology)\n",
      "- Page 21 (Section: introduction_to_psychology/history_of_psychology)\n",
      "- Page 33 (Section: introduction_to_psychology/contemporary_psychology)\n",
      "\n",
      "Sections referenced:\n",
      "- introduction_to_psychology/contemporary_psychology\n",
      "- introduction_to_psychology/history_of_psychology\n",
      "- introduction_to_psychology/what_is_psychology\n"
     ]
    }
   ],
   "source": [
    "# Main RAG pipeline\n",
    "def process_query(query: str) -> FinalResponse:\n",
    "    \"\"\"Process a query through the full RAG pipeline\"\"\"\n",
    "    # 1. Generate query variations using LLM\n",
    "    variations = generate_query_variations(query)\n",
    "    print(f\"Generated {len(variations)} query variations:\")\n",
    "    for i, var in enumerate(variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    \n",
    "    # 2. Get results for each variation\n",
    "    all_results = []\n",
    "    variation_results = []\n",
    "    \n",
    "    for var in variations:\n",
    "        # Get initial results\n",
    "        results = vectorstore.similarity_search_with_score(var, k=5)\n",
    "        \n",
    "        # Rerank results\n",
    "        pairs = [[var, doc.page_content] for doc, _ in results]\n",
    "        rerank_scores = reranker.compute_score(pairs)\n",
    "        \n",
    "        # Create validated chunks\n",
    "        chunks = [\n",
    "            RetrievedChunk(\n",
    "                text=doc.page_content,\n",
    "                metadata=ChunkMetadata(\n",
    "                    chunk_id=doc.metadata[\"chunk_id\"],\n",
    "                    physical_page=doc.metadata[\"physical_page\"],\n",
    "                    logical_page=doc.metadata[\"logical_page\"],\n",
    "                    section=doc.metadata[\"section\"]\n",
    "                ),\n",
    "                score=float(score)\n",
    "            )\n",
    "            for (doc, _), score in zip(results, rerank_scores)\n",
    "        ]\n",
    "        \n",
    "        # Add to variation results\n",
    "        variation_results.append(\n",
    "            QueryVariation(\n",
    "                query=var,\n",
    "                chunks=chunks[:5]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add to combined results for RRF\n",
    "        all_results.extend([(doc, score) for (doc, _), score in zip(results, rerank_scores)])\n",
    "    \n",
    "    # 3. Apply RRF to get final chunks\n",
    "    final_chunks = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    # 4. Extract unique sections\n",
    "    sections = list(set(\n",
    "        doc.metadata['section'] \n",
    "        for doc, _ in final_chunks\n",
    "    ))\n",
    "    \n",
    "    # 5. Generate answer using LLM\n",
    "    final_docs = [doc for doc, _ in final_chunks]\n",
    "    answer = generate_answer(query, final_docs)\n",
    "    \n",
    "    # 6. Create and validate final response\n",
    "    response = FinalResponse(\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        source_chunks=[\n",
    "            ChunkMetadata(\n",
    "                chunk_id=doc.metadata['chunk_id'],\n",
    "                physical_page=doc.metadata['physical_page'],\n",
    "                logical_page=doc.metadata['logical_page'],\n",
    "                section=doc.metadata['section']\n",
    "            )\n",
    "            for doc, _ in final_chunks\n",
    "        ],\n",
    "        sections_referenced=sections,\n",
    "        query_variations=variations\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the pipeline\n",
    "def test_pipeline():\n",
    "    \"\"\"Test the RAG pipeline with a sample query\"\"\"\n",
    "    query = \"What is psychology?\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    response = process_query(query)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Query: {response.query}\")\n",
    "    print(\"\\nQuery variations used:\")\n",
    "    for i, var in enumerate(response.query_variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(\"\\nSource chunks:\")\n",
    "    for chunk in response.source_chunks:\n",
    "        print(f\"- Page {chunk.physical_page} (Section: {chunk.section})\")\n",
    "    print(\"\\nSections referenced:\")\n",
    "    for section in response.sections_referenced:\n",
    "        print(f\"- {section}\")\n",
    "\n",
    "# Run test\n",
    "test_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09e119ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is psychology?\n",
      "Generated 5 query variations:\n",
      "0. What is psychology?\n",
      "1. How would you define psychology in academic terms?\n",
      "2. What are the main concepts and principles of psychology?\n",
      "3. Can you explain the fundamental aspects of psychology?\n",
      "4. What are the key characteristics that define psychology?\n",
      "\n",
      "Results:\n",
      "Query: What is psychology?\n",
      "\n",
      "Query variations used:\n",
      "0. What is psychology?\n",
      "1. How would you define psychology in academic terms?\n",
      "2. What are the main concepts and principles of psychology?\n",
      "3. Can you explain the fundamental aspects of psychology?\n",
      "4. What are the key characteristics that define psychology?\n",
      "\n",
      "Answer: [Page 20 - introduction_to_psychology/what_is_psycho]: and it cannot arrive at knowledge about values and morality. This is one reason why our scientific understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. The scientific method is also a form of empiricism. An empirical method for acquiring knowledge is one based on observation, including experimentation, and the study of psychology.\n",
      "\n",
      "Source chunks:\n",
      "- Page 20 (Section: introduction_to_psychology/what_is_psychology)\n",
      "- Page 21 (Section: introduction_to_psychology/history_of_psychology)\n",
      "- Page 20 (Section: introduction_to_psychology/what_is_psychology)\n",
      "\n",
      "Sections referenced:\n",
      "- introduction_to_psychology/history_of_psychology\n",
      "- introduction_to_psychology/what_is_psychology\n"
     ]
    }
   ],
   "source": [
    "# Updated RAG pipeline\n",
    "def retrieve_and_rerank(query: str, vectorstore, reranker, top_k: int = 5) -> List[tuple]:\n",
    "    \"\"\"Retrieve and rerank chunks for a single query\"\"\"\n",
    "    # Get initial results with more candidates for reranking\n",
    "    results = vectorstore.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k * 2  # Get more candidates for better reranking\n",
    "    )\n",
    "    \n",
    "    # Rerank\n",
    "    pairs = [[query, doc.page_content] for doc, _ in results]\n",
    "    rerank_scores = reranker.compute_score(pairs)\n",
    "    \n",
    "    # Sort by reranker score\n",
    "    scored_results = [(doc, float(score)) for (doc, _), score in zip(results, rerank_scores)]\n",
    "    scored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_results[:top_k]\n",
    "\n",
    "def process_query(query: str) -> FinalResponse:\n",
    "    \"\"\"Process a query through the full RAG pipeline\"\"\"\n",
    "    # 1. Generate query variations\n",
    "    variations = generate_query_variations(query)\n",
    "    print(f\"Generated {len(variations)} query variations:\")\n",
    "    for i, var in enumerate(variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    \n",
    "    # 2. Get results for each variation\n",
    "    all_results = []\n",
    "    for var in variations:\n",
    "        results = retrieve_and_rerank(var, vectorstore, reranker)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 3. Apply RRF to get final chunks\n",
    "    final_chunks = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    # 4. Extract unique sections\n",
    "    sections = list(set(\n",
    "        doc.metadata['section'] \n",
    "        for doc, _ in final_chunks\n",
    "    ))\n",
    "    \n",
    "    # 5. Generate answer using LLM\n",
    "    final_docs = [doc for doc, _ in final_chunks]\n",
    "    answer = generate_answer(query, final_docs)\n",
    "    \n",
    "    # 6. Create final response\n",
    "    response = FinalResponse(\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        source_chunks=[\n",
    "            ChunkMetadata(\n",
    "                chunk_id=doc.metadata['chunk_id'],\n",
    "                physical_page=doc.metadata['physical_page'],\n",
    "                logical_page=doc.metadata['logical_page'],\n",
    "                section=doc.metadata['section']\n",
    "            )\n",
    "            for doc, _ in final_chunks\n",
    "        ],\n",
    "        sections_referenced=sections,\n",
    "        query_variations=variations\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the pipeline\n",
    "def test_pipeline():\n",
    "    \"\"\"Test the RAG pipeline with a sample query\"\"\"\n",
    "    query = \"What is psychology?\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    response = process_query(query)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Query: {response.query}\")\n",
    "    print(\"\\nQuery variations used:\")\n",
    "    for i, var in enumerate(response.query_variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(\"\\nSource chunks:\")\n",
    "    for chunk in response.source_chunks:\n",
    "        print(f\"- Page {chunk.physical_page} (Section: {chunk.section})\")\n",
    "    print(\"\\nSections referenced:\")\n",
    "    for section in response.sections_referenced:\n",
    "        print(f\"- {section}\")\n",
    "\n",
    "# Run test\n",
    "test_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1865e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sections mapping...\n",
      "Loaded 494 page-to-section mappings\n"
     ]
    }
   ],
   "source": [
    "# Load section mapping\n",
    "def load_section_mapping():\n",
    "    \"\"\"Load and prepare section mapping\"\"\"\n",
    "    print(\"Loading sections mapping...\")\n",
    "    with open(SECTIONS_JSON_PATH, \"r\") as f:\n",
    "        sections_map = json.load(f)\n",
    "    \n",
    "    # Ensure all keys are strings\n",
    "    sections_map = {str(k): v for k, v in sections_map.items()}\n",
    "    print(f\"Loaded {len(sections_map)} page-to-section mappings\")\n",
    "    return sections_map\n",
    "\n",
    "# Text cleaning\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text, removing supplementary sections\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove supplementary sections\n",
    "    sections_to_remove = [\n",
    "        r'Key Terms\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Summary\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Review Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Critical Thinking Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Personal Application Questions\\s.*?(?=\\n\\n|\\Z)',\n",
    "        r'Access for free at openstax\\.org\\.*',\n",
    "        r'LINK TO LEARNING.*?(?=\\n|$)',\n",
    "        r'Watch a brief video.*?(?=\\n|$)',\n",
    "        r'http[s]?://\\S+'\n",
    "    ]\n",
    "    \n",
    "    for pattern in sections_to_remove:\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Load sections mapping\n",
    "sections_map = load_section_mapping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65032ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Processing pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 753/753 [00:00<00:00, 2456.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 2364 chunks from 494 valid pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Document processing\n",
    "def process_documents():\n",
    "    \"\"\"Load and process PDF, only including mapped pages\"\"\"\n",
    "    # Load documents\n",
    "    print(\"Loading PDF...\")\n",
    "    loader = PyPDFLoader(str(BOOK_PDF_PATH))\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Get valid pages\n",
    "    valid_logical_pages = set(int(page) for page in sections_map.keys())\n",
    "    chunks = []\n",
    "    \n",
    "    # Process documents\n",
    "    print(\"Processing pages...\")\n",
    "    for doc in tqdm(documents, desc=\"Processing pages\"):\n",
    "        physical_page = int(doc.metadata.get(\"page\", 0)) + 1\n",
    "        logical_page = physical_page - LOGICAL_PAGE_OFFSET\n",
    "        \n",
    "        # Skip if not in mapping\n",
    "        if logical_page not in valid_logical_pages:\n",
    "            continue\n",
    "        \n",
    "        # Get section and clean text\n",
    "        section = sections_map[str(logical_page)]\n",
    "        cleaned_text = clean_text(doc.page_content)\n",
    "        \n",
    "        # Skip if too short\n",
    "        if len(cleaned_text.split()) < 20:\n",
    "            continue\n",
    "            \n",
    "        # Create chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        doc_chunks = splitter.split_text(cleaned_text)\n",
    "        \n",
    "        # Add chunks with metadata\n",
    "        for i, chunk_text in enumerate(doc_chunks):\n",
    "            chunk_id = f\"p{physical_page}_w{i:03d}\"\n",
    "            chunks.append(Document(\n",
    "                page_content=chunk_text,\n",
    "                metadata={\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"physical_page\": physical_page,\n",
    "                    \"logical_page\": logical_page,\n",
    "                    \"section\": section\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    print(f\"\\nCreated {len(chunks)} chunks from {len(valid_logical_pages)} valid pages\")\n",
    "    return chunks\n",
    "\n",
    "# Process documents\n",
    "chunks = process_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb85231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up embedding model...\n",
      "Creating vector store...\n",
      "Adding documents...\n",
      "Initializing reranker...\n"
     ]
    }
   ],
   "source": [
    "# Setup vector store and reranker\n",
    "def setup_vector_store(chunks: List[Document]):\n",
    "    \"\"\"Initialize and populate vector store\"\"\"\n",
    "    print(\"Setting up embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': DEVICE},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=str(CHROMA_PATH),\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"Adding documents...\")\n",
    "    vectorstore.add_documents(chunks)\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore, embeddings\n",
    "\n",
    "# Initialize reranker\n",
    "def setup_reranker():\n",
    "    \"\"\"Initialize the reranker model\"\"\"\n",
    "    print(\"Initializing reranker...\")\n",
    "    return FlagReranker(\n",
    "        RERANKER_MODEL,\n",
    "        use_fp16=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# Setup models\n",
    "vectorstore, embeddings = setup_vector_store(chunks)\n",
    "reranker = setup_reranker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7c2886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Flan-T5-small model...\n",
      "LLM setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup LLM for generation\n",
    "print(\"Loading Flan-T5-small model...\")\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "def generate_response(query: str, context: str) -> str:\n",
    "    \"\"\"Generate response using T5 model\"\"\"\n",
    "    prompt = f\"\"\"Answer the question based on the given context. Be clear and concise.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Use only information from the context\n",
    "    2. Be specific and accurate\n",
    "    3. If context doesn't contain enough information, say so\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        min_length=50,\n",
    "        temperature=0.7,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"LLM setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65247068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query expansion and retrieval\n",
    "def generate_query_variations(query: str) -> List[str]:\n",
    "    \"\"\"Generate query variations\"\"\"\n",
    "    variations = [\n",
    "        query,  # Original query\n",
    "        f\"Define {query.lower().replace('what is ', '')}\",\n",
    "        f\"Explain the concept of {query.lower().replace('what is ', '')}\",\n",
    "        f\"Describe {query.lower().replace('what is ', '')}\",\n",
    "        f\"Tell me about {query.lower().replace('what is ', '')}\"\n",
    "    ]\n",
    "    return variations\n",
    "\n",
    "def reciprocal_rank_fusion(all_results: List[tuple], k: int = 60) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Combine results using Reciprocal Rank Fusion\n",
    "    \n",
    "    Args:\n",
    "        all_results: List of (Document, score) tuples\n",
    "        k: Constant to prevent division by zero and smooth impact\n",
    "    \"\"\"\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    # Calculate RRF scores\n",
    "    for rank, (doc, score) in enumerate(all_results):\n",
    "        doc_id = doc.metadata['chunk_id']\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1/(rank + k)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    sorted_docs = sorted(\n",
    "        [(doc, score) for doc, score in all_results],\n",
    "        key=lambda x: rrf_scores[x[0].metadata['chunk_id']],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Take top 3 unique chunks\n",
    "    seen_ids = set()\n",
    "    final_results = []\n",
    "    for doc, score in sorted_docs:\n",
    "        if doc.metadata['chunk_id'] not in seen_ids:\n",
    "            seen_ids.add(doc.metadata['chunk_id'])\n",
    "            final_results.append((doc, score))\n",
    "            if len(final_results) == 3:\n",
    "                break\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def retrieve_and_rerank(query: str, vectorstore, reranker, top_k: int = 5) -> QueryResults:\n",
    "    \"\"\"Retrieve and rerank chunks for a single query\"\"\"\n",
    "    # Get initial results\n",
    "    results = vectorstore.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k\n",
    "    )\n",
    "    \n",
    "    # Rerank\n",
    "    pairs = [[query, doc.page_content] for doc, _ in results]\n",
    "    rerank_scores = reranker.compute_score(pairs)\n",
    "    \n",
    "    # Create validated results\n",
    "    chunks = [\n",
    "        RetrievedChunk(\n",
    "            text=doc.page_content,\n",
    "            metadata=ChunkMetadata(\n",
    "                chunk_id=doc.metadata[\"chunk_id\"],\n",
    "                physical_page=doc.metadata[\"physical_page\"],\n",
    "                logical_page=doc.metadata[\"logical_page\"],\n",
    "                section=doc.metadata[\"section\"]\n",
    "            ),\n",
    "            score=float(score)\n",
    "        )\n",
    "        for (doc, _), score in zip(results, rerank_scores)\n",
    "    ]\n",
    "    \n",
    "    return QueryResults(\n",
    "        query=query,\n",
    "        chunks=chunks[:5]  # Ensure exactly 5 chunks\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bafb5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What is psychology?\n",
      "\n",
      "Generated variations:\n",
      "1. 1. Answer the question. 2. Be specific and accurate. 3. if you don't have enough information, say so. 4. If you have a question, you can answer it. 5. If the answer is no, it's not possible to answer.\n"
     ]
    }
   ],
   "source": [
    "# Query expansion using LLM\n",
    "def generate_query_variations(query: str) -> List[str]:\n",
    "    \"\"\"Generate query variations using LLM\"\"\"\n",
    "    prompt = f\"\"\"Generate 4 different ways to ask this question. The variations should help in retrieving relevant information.\n",
    "    \n",
    "    Original question: {query}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Keep the same meaning but use different phrasings\n",
    "    2. Consider different aspects of the topic\n",
    "    3. Make variations specific and focused\n",
    "    4. Return exactly 4 variations\n",
    "    \n",
    "    Format each variation on a new line.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate variations\n",
    "    response = generate_response(query=query, context=prompt)\n",
    "    \n",
    "    # Split into lines and clean\n",
    "    variations = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "    variations = variations[:4]  # Ensure exactly 4 variations\n",
    "    \n",
    "    # Add original query and ensure we have exactly 5 queries\n",
    "    all_queries = [query] + variations\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_queries[:5]  # Return exactly 5 queries\n",
    "\n",
    "# Example usage\n",
    "test_query = \"What is psychology?\"\n",
    "variations = generate_query_variations(test_query)\n",
    "print(\"Original:\", test_query)\n",
    "print(\"\\nGenerated variations:\")\n",
    "for i, var in enumerate(variations[1:], 1):\n",
    "    print(f\"{i}. {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4613887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is psychology?\n",
      "Generated 2 query variations:\n",
      "0. What is psychology?\n",
      "1. 1. Answer the question. 2. Be specific and accurate. 3. if you don't have enough information, say so. 4. If you have a question, you can answer it. 5. If the answer is no, it's not possible to answer.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for QueryResults\nchunks.0\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.1\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.2\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.3\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='revo...gy'), score=-1.37890625), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.4\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='revo...gy'), score=-1.37890625), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Run test\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtest_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 63\u001b[0m, in \u001b[0;36mtest_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is psychology?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 14\u001b[0m, in \u001b[0;36mprocess_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variations:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Get results for this variation\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_and_rerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreranker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Keep the Document objects intact\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     initial_results \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(var, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 74\u001b[0m, in \u001b[0;36mretrieve_and_rerank\u001b[0;34m(query, vectorstore, reranker, top_k)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Create validated results\u001b[39;00m\n\u001b[1;32m     60\u001b[0m chunks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     61\u001b[0m     RetrievedChunk(\n\u001b[1;32m     62\u001b[0m         text\u001b[38;5;241m=\u001b[39mdoc\u001b[38;5;241m.\u001b[39mpage_content,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (doc, _), score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, rerank_scores)\n\u001b[1;32m     72\u001b[0m ]\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQueryResults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure exactly 5 chunks\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for QueryResults\nchunks.0\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.1\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.2\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='2002...ology'), score=3.703125), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.3\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='revo...gy'), score=-1.37890625), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type\nchunks.4\n  Input should be a valid dictionary or instance of RetrievedChunk [type=model_type, input_value=RetrievedChunk(text='revo...gy'), score=-1.37890625), input_type=RetrievedChunk]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "# Main RAG pipeline\n",
    "def process_query(query: str) -> FinalResponse:\n",
    "    \"\"\"Process a query through the full RAG pipeline\"\"\"\n",
    "    # 1. Generate query variations using LLM\n",
    "    variations = generate_query_variations(query)\n",
    "    print(f\"Generated {len(variations)} query variations:\")\n",
    "    for i, var in enumerate(variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    \n",
    "    # 2. Get results for each variation\n",
    "    all_results = []\n",
    "    for var in variations:\n",
    "        # Get results for this variation\n",
    "        results = retrieve_and_rerank(var, vectorstore, reranker)\n",
    "        \n",
    "        # Keep the Document objects intact\n",
    "        initial_results = vectorstore.similarity_search_with_score(var, k=5)\n",
    "        for doc, base_score in initial_results:\n",
    "            # Find matching reranked score\n",
    "            for chunk in results.chunks:\n",
    "                if chunk.metadata.chunk_id == doc.metadata['chunk_id']:\n",
    "                    all_results.append((doc, chunk.score))\n",
    "                    break\n",
    "    \n",
    "    # 3. Apply RRF to get final chunks\n",
    "    final_chunks = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    # 4. Extract unique sections\n",
    "    sections = list(set(\n",
    "        doc.metadata['section'] \n",
    "        for doc, _ in final_chunks\n",
    "    ))\n",
    "    \n",
    "    # 5. Generate answer using LLM\n",
    "    final_docs = [doc for doc, _ in final_chunks]\n",
    "    answer = generate_answer(query, final_docs)\n",
    "    \n",
    "    # 6. Create and validate final response\n",
    "    response = FinalResponse(\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        source_chunks=[\n",
    "            ChunkMetadata(\n",
    "                chunk_id=doc.metadata['chunk_id'],\n",
    "                physical_page=doc.metadata['physical_page'],\n",
    "                logical_page=doc.metadata['logical_page'],\n",
    "                section=doc.metadata['section']\n",
    "            )\n",
    "            for doc, _ in final_chunks\n",
    "        ],\n",
    "        sections_referenced=sections,\n",
    "        query_variations=variations\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the pipeline\n",
    "def test_pipeline():\n",
    "    \"\"\"Test the RAG pipeline with a sample query\"\"\"\n",
    "    query = \"What is psychology?\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    \n",
    "    response = process_query(query)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Query: {response.query}\")\n",
    "    print(\"\\nQuery variations used:\")\n",
    "    for i, var in enumerate(response.query_variations):\n",
    "        print(f\"{i}. {var}\")\n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(\"\\nSource chunks:\")\n",
    "    for chunk in response.source_chunks:\n",
    "        print(f\"- Page {chunk.physical_page} (Section: {chunk.section})\")\n",
    "    print(\"\\nSections referenced:\")\n",
    "    for section in response.sections_referenced:\n",
    "        print(f\"- {section}\")\n",
    "\n",
    "# Run test\n",
    "test_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
