{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f7c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation and Imports\n",
    "!pip -q install langchain langchain-community langchain-text-splitters\n",
    "!pip -q install pypdf pymupdf sentence-transformers\n",
    "!pip -q install numpy pandas tqdm faiss-cpu\n",
    "!pip -q install instructor pydantic transformers accelerate torch\n",
    "\n",
    "# Imports\n",
    "from pathlib import Path\n",
    "import json, re, pickle\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LLM imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from instructor import patch\n",
    "\n",
    "# Pydantic for validation\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Optional: keep CPU thread usage sane in notebooks\n",
    "if not torch.cuda.is_available():\n",
    "    torch.set_num_threads(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57267dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sections mapping...\n",
      "Loaded 494 page-to-section mappings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 620/620 [00:00<00:00, 7450.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1780\n",
      "------------------------------------------------------------\n",
      "id     : p19_w000\n",
      "page   : phys 19 | logical 7\n",
      "section: (none)\n",
      "text   : FIGURE 1.1 Psychology is the scientific study of mind and behavior. (credit \"background\": modification of work by Nattachai Noogure; credit \"top left\": modification of work by Peter Shanks; credit \"top middle\": modification of work by \"devinf\"/Flickr; credit \"top right\": modification of work by Alejandra Quintero Sinisterra; credit \"bottom left\": modification of work by Gabriel Rocha; credit \"bottom middle-left\": modification of work by Caleb Roenigk; credit \"bottom middle-right\": modification of work by Staffan Scherz; credit \"bottom right\": modification of work by Czech Provincial Reconstruction Team) INTRODUCTION CHAPTER OUTLINE 1.1 What Is Psychology? 1.2 History of Psychology 1.3 Contemporary Psychology 1.4 Careers in Psychology Clive Wearing is an accomplished musician who lost his ability to form new memories when he became sick at the age of 46. While he can remember how to play the piano perfectly, he cannot remember what he ate for breakfast just an hour ago (Sacks, 2007). James Wannerton experiences a taste sensation that is associated with the sound of words. His former girlfriend’s name tastes like rhubarb (Mundasad, 2013). John Nash is a brilliant mathematician and Nobel Prize winner. However, while he was a professor at MIT, he would tell people that the New York Times contained coded messages from extraterrestrial beings that were intended for him. He also began to hear voices and became suspicious of...\n",
      "------------------------------------------------------------\n",
      "id     : p19_w001\n",
      "page   : phys 19 | logical 7\n",
      "section: (none)\n",
      "text   : Prize winner. However, while he was a professor at MIT, he would tell people that the New York Times contained coded messages from extraterrestrial beings that were intended for him. He also began to hear voices and became suspicious of the people around him. Soon thereafter, Nash was diagnosed with schizophrenia and admitted to a state-run mental institution (O’Connor & Robertson, 1Introduction to Psychology...\n",
      "------------------------------------------------------------\n",
      "id     : p20_w000\n",
      "page   : phys 20 | logical 8\n",
      "section: introduction_to_psychology/what_is_psychology\n",
      "text   : 2002). Nash was the subject of the 2001 movie A Beautiful Mind. Why did these people have these experiences? How does the human brain work? And what is the connection between the brain’s internal processes and people’s external behaviors? This textbook will introduce you to various ways that the field of psychology has explored these questions. 1.1 What Is Psychology? LEARNING OBJECTIVES By the end of this section, you will be able to: • Define psychology • Understand the merits of an education in psychology What is creativity? What are prejudice and discrimination? What is consciousness? The field of psychology explores questions like these. Psychology refers to the scientific study of the mind and behavior. Psychologists use the scientific method to acquire knowledge. To apply the scientific method, a researcher with a question about how or why something happens will propose a tentative explanation, called a hypothesis, to explain the phenomenon. A hypothesis should fit into the context of a scientific theory, which is a broad explanation or group of explanations for some aspect of the natural world that is consistently supported by evidence over time. A theory is the best understanding we have of that part of the natural world. The researcher then makes observations or carries out an experiment to test the validity of the hypothesis. Those results...\n",
      "------------------------------------------------------------\n",
      "id     : p20_w001\n",
      "page   : phys 20 | logical 8\n",
      "section: introduction_to_psychology/what_is_psychology\n",
      "text   : consistently supported by evidence over time. A theory is the best understanding we have of that part of the natural world. The researcher then makes observations or carries out an experiment to test the validity of the hypothesis. Those results are then published or presented at research conferences so that others can replicate or build on the results. Scientists test that which is perceivable and measurable. For example, the hypothesis that a bird sings because it is happy is not a hypothesis that can be tested since we have no way to measure the happiness of a bird. We must ask a different question, perhaps about the brain state of the bird, since this can be measured. However, we can ask individuals about whether they sing because they are happy since they are able to tell us. Thus, psychological science is empirical, based on measurable data. In general, science deals only with matter and energy, that is, those things that can be measured, and it cannot arrive at knowledge about values and morality. This is one reason why our scientific understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. The scientific method is also a form of empiricism. An empirical method for acquiring knowledge is one based on observation,...\n",
      "------------------------------------------------------------\n",
      "id     : p20_w002\n",
      "page   : phys 20 | logical 8\n",
      "section: introduction_to_psychology/what_is_psychology\n",
      "text   : understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. The scientific method is also a form of empiricism. An empirical method for acquiring knowledge is one based on observation, including experimentation, rather than a method based only on forms of logical argument or previous authorities. It was not until the late 1800s that psychology became accepted as its own academic discipline. Before this time, the workings of the mind were considered under the auspices of philosophy. Given that any behavior is, at its roots, biological, some areas of psychology take on aspects of a natural science like biology. No biological organism exists in isolation, and our behavior is influenced by our interactions with others. Therefore, psychology is also a social science. WHY STUDY PSYCHOLOGY? Often, students take their first psychology course because they are interested in helping others and want to learn more about themselves and why they act the way they do. Sometimes, students take a psychology course because it either satisfies a general education requirement or is required for a program of study such as nursing or pre-med. Many of these students develop such an interest in the area that they go on to declare psychology as their major. As a result, psychology is one of the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BOOK_PDF_PATH = Path(\"../Sources/book.pdf\")\n",
    "SECTIONS_JSON_PATH = Path(\"./working/page_to_section.json\")\n",
    "\n",
    "# Verify files exist\n",
    "assert BOOK_PDF_PATH.exists(), f\"PDF not found: {BOOK_PDF_PATH}\"\n",
    "assert SECTIONS_JSON_PATH.exists(), f\"Sections JSON not found: {SECTIONS_JSON_PATH}\"\n",
    "\n",
    "# Load sections mapping\n",
    "print(\"Loading sections mapping...\")\n",
    "with open(SECTIONS_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    sections_map = json.load(f)\n",
    "\n",
    "# Ensure all keys are strings\n",
    "sections_map = {str(k): v for k, v in sections_map.items()}\n",
    "print(f\"Loaded {len(sections_map)} page-to-section mappings\")\n",
    "def clean_page_text(text: str) -> str:\n",
    "    \"\"\"Light cleaning for a single page string.\"\"\"\n",
    "    if not text: \n",
    "        return \"\"\n",
    "    \n",
    "    # Remove common boilerplate text\n",
    "    text = re.sub(r'Access for free at openstax\\.org\\.*', '', text)\n",
    "    text = re.sub(r'LINK TO LEARNING.*?(?=\\n|$)', '', text)  # Remove LINK TO LEARNING sections\n",
    "    text = re.sub(r'Watch a brief video.*?(?=\\n|$)', '', text)  # Remove video references\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # Remove URLs\n",
    "    \n",
    "    # Basic text normalization\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"-\\n(\\w)\", r\"\\1\", text)       # de-hyphenate linebreaks\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)       # strip trailing spaces before newline\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)       # collapse multi-spaces\n",
    "    \n",
    "    # Remove multiple consecutive newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def split_into_word_windows(text: str, size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Split ONE page into overlapping word windows. Returns list of strings.\"\"\"\n",
    "    words = text.replace(\"\\n\", \" \").split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(words):\n",
    "        # Take size words for this chunk\n",
    "        chunk = words[start_idx:start_idx + size]\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        \n",
    "        # Move start_idx forward by (size - overlap) to create overlap\n",
    "        start_idx += (size - overlap)\n",
    "        \n",
    "        # If we can't make a full chunk anymore, break\n",
    "        if start_idx + size > len(words):\n",
    "            # Add final chunk if there are remaining words\n",
    "            remaining = words[start_idx:]\n",
    "            if remaining:\n",
    "                chunks.append(\" \".join(remaining))\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(physical_page: int, idx: int) -> str:\n",
    "    \"\"\"Make a stable chunk id like 'p19_w003' that preserves page number.\"\"\"\n",
    "    return f\"p{physical_page}_w{idx:03d}\"\n",
    "\n",
    "def process_pdf(pdf_path: str, \n",
    "               sections_map: dict,\n",
    "               start_page: int = 19,    # Skip front matter\n",
    "               end_page: int = 638,     # Stop before references\n",
    "               logical_offset: int = 12,\n",
    "               chunk_size: int = 220,    \n",
    "               chunk_overlap: int = 40    \n",
    "               ) -> List[Document]:\n",
    "    \"\"\"Process PDF while maintaining page boundaries and creating overlapping chunks.\"\"\"\n",
    "    \n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    docs_all = loader.load()\n",
    "    \n",
    "    # Filter pages between start_page and end_page\n",
    "    page_docs = [d for d in docs_all if start_page <= int(d.metadata.get(\"page\", 0)) + 1 <= end_page]\n",
    "    chunks = []\n",
    "    \n",
    "    for d in tqdm(page_docs, desc=\"Processing pages\"):\n",
    "        physical_page = int(d.metadata.get(\"page\", 0)) + 1\n",
    "        logical_page = physical_page - logical_offset\n",
    "        section = sections_map.get(str(logical_page), \"\")\n",
    "        \n",
    "        page_text = clean_page_text(d.page_content)\n",
    "        windows = split_into_word_windows(\n",
    "            text=page_text,\n",
    "            size=chunk_size,\n",
    "            overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        for i, window in enumerate(windows):\n",
    "            metadata = {\n",
    "                \"chunk_id\": make_chunk_id(physical_page, i),\n",
    "                \"physical_page\": physical_page,\n",
    "                \"logical_page\": logical_page,\n",
    "                \"section\": section,\n",
    "                \"source\": str(pdf_path),\n",
    "                \"chunk_size_words\": chunk_size,\n",
    "                \"chunk_overlap_words\": chunk_overlap,\n",
    "            }\n",
    "            chunks.append(Document(page_content=window, metadata=metadata))\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Usage\n",
    "chunks = process_pdf(\n",
    "    pdf_path=BOOK_PDF_PATH,\n",
    "    sections_map=sections_map,\n",
    "    start_page=19,   # Skip front matter\n",
    "    end_page=638    # Stop before references\n",
    ")\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "for c in chunks[:5]:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"id     : {c.metadata['chunk_id']}\")\n",
    "    print(f\"page   : phys {c.metadata['physical_page']} | logical {c.metadata['logical_page']}\")\n",
    "    print(f\"section: {c.metadata['section'] or '(none)'}\")\n",
    "    print(f\"text   : {c.page_content}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9a1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/ns3sdnhj22n2q3hsq7mpbgv40000gn/T/ipykernel_7414/2111709763.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "\n",
      "Building FAISS index from chunks...\n",
      "\n",
      "Saving index to artifacts/faiss_index...\n",
      "\n",
      "Verifying saved index...\n",
      "\n",
      "Test Results:\n",
      "Query: What is psychology?\n",
      "\n",
      "Result 1:\n",
      "Page: 20 (logical: 8)\n",
      "Section: introduction_to_psychology/what_is_psychology\n",
      "Text: understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. The scientific method is also a form of empiricism. An empirical method for acqu...\n",
      "\n",
      "Result 2:\n",
      "Page: 20 (logical: 8)\n",
      "Section: introduction_to_psychology/what_is_psychology\n",
      "Text: 2002). Nash was the subject of the 2001 movie A Beautiful Mind. Why did these people have these experiences? How does the human brain work? And what is the connection between the brain’s internal proc...\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: EMBEDDINGS AND VECTOR STORE\n",
    "\"\"\"\n",
    "In this step, we create a semantic search index for our text chunks:\n",
    "\n",
    "1. Create embeddings using HuggingFace's sentence-transformers\n",
    "   - Using all-MiniLM-L6-v2: Efficient model that works well on CPU\n",
    "   - Normalized embeddings for better cosine similarity\n",
    "\n",
    "2. Build FAISS index for fast similarity search\n",
    "   - FAISS is efficient for large-scale similarity search\n",
    "   - Works well on CPU for our dataset size\n",
    "\n",
    "3. Save index for reuse\n",
    "   - Persists both the FAISS index and metadata\n",
    "   - Allows quick reloading without recomputing embeddings\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Efficient CPU model\n",
    "INDEX_SAVE_PATH = Path(\"./artifacts/faiss_index\")                # Where to save the index\n",
    "INDEX_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},           # Force CPU - works well for this model\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # Better for cosine similarity\n",
    ")\n",
    "\n",
    "# Create FAISS index from our chunks\n",
    "print(\"\\nBuilding FAISS index from chunks...\")\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,           # Our preprocessed text chunks\n",
    "    embedding=embeddings       # The embedding model\n",
    ")\n",
    "\n",
    "# Save index for reuse\n",
    "print(f\"\\nSaving index to {INDEX_SAVE_PATH}...\")\n",
    "vectorstore.save_local(str(INDEX_SAVE_PATH))\n",
    "\n",
    "# Quick verification of saved index\n",
    "print(\"\\nVerifying saved index...\")\n",
    "reloaded_vectorstore = FAISS.load_local(\n",
    "    folder_path=str(INDEX_SAVE_PATH),\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required in notebooks\n",
    ")\n",
    "\n",
    "# Test a simple query to verify everything works\n",
    "test_query = \"What is psychology?\"\n",
    "test_results = reloaded_vectorstore.similarity_search(\n",
    "    query=test_query,\n",
    "    k=2  # Get top 2 results\n",
    ")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Query: {test_query}\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Page: {doc.metadata['physical_page']} (logical: {doc.metadata['logical_page']})\")\n",
    "    print(f\"Section: {doc.metadata['section']}\")\n",
    "    print(f\"Text: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60df3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FLAN-T5-small model...\n",
      "\n",
      "Original query: What is psychology?\n",
      "\n",
      "Expanded queries:\n",
      "1. What is psychology?\n",
      "2. Define psychology?\n",
      "3. Explain the concept of psychology?\n",
      "4. Describe psychology?\n",
      "\n",
      "Saving expanded queries...\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: MULTI-QUERY EXPANSION WITH FLAN-T5\n",
    "\"\"\"\n",
    "In this step, we implement query expansion using FLAN-T5-small:\n",
    "1. Use FLAN-T5: Very lightweight model optimized for instructions\n",
    "2. Works efficiently on CPU without quantization\n",
    "3. Generate 4 semantically similar queries\n",
    "4. Validate output format with Pydantic\n",
    "\n",
    "Why FLAN-T5-small?\n",
    "- Only 80M parameters\n",
    "- Optimized for instruction following\n",
    "- Works well on CPU without special optimizations\n",
    "- Very low memory footprint (~300MB RAM)\n",
    "- Fast inference time\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Setup directories\n",
    "artifacts_dir = Path(\"./artifacts\")\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class QueryExpansion(BaseModel):\n",
    "    \"\"\"Validates the structure of expanded queries.\"\"\"\n",
    "    queries: List[str] = Field(\n",
    "        ...,\n",
    "        min_items=4,\n",
    "        max_items=4,\n",
    "        description=\"List of 4 semantically similar but differently phrased queries\"\n",
    "    )\n",
    "    \n",
    "    @field_validator(\"queries\")\n",
    "    def validate_queries(cls, queries: List[str]) -> List[str]:\n",
    "        # Remove empty strings and whitespace\n",
    "        queries = [q.strip() for q in queries if q.strip()]\n",
    "        \n",
    "        if len(queries) != 4:\n",
    "            raise ValueError(\"Must have exactly 4 non-empty queries\")\n",
    "            \n",
    "        # Check for duplicates\n",
    "        if len(set(queries)) != len(queries):\n",
    "            raise ValueError(\"All queries must be unique\")\n",
    "            \n",
    "        return queries\n",
    "\n",
    "# Prompt template for T5\n",
    "INSTRUCTION_TEMPLATE = \"\"\"Generate 4 different ways to ask this question while keeping the same meaning. Return a JSON object.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Requirements:\n",
    "- Keep the same meaning and scope\n",
    "- Use different vocabulary and phrasings\n",
    "- Make each version unique\n",
    "- Return exactly 4 versions\n",
    "- Focus on semantic variation\n",
    "\n",
    "Return format:\n",
    "{{\n",
    "  \"queries\": [\n",
    "    \"variation 1\",\n",
    "    \"variation 2\",\n",
    "    \"variation 3\",\n",
    "    \"variation 4\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "JSON output:\"\"\"\n",
    "\n",
    "print(\"Loading FLAN-T5-small model...\")\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "def generate_response(prompt: str, input_query: str, max_attempts: int = 3) -> str:\n",
    "    \"\"\"Generate response from T5 model with retries.\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # T5 specific tokenization\n",
    "            inputs = tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=512, \n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # T5 specific generation\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=150,  # Shorter for JSON output\n",
    "                min_length=50,   # Ensure some content\n",
    "                temperature=0.6 + (attempt * 0.1),  # Start cooler\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                repetition_penalty=1.2,  # Avoid repeating phrases\n",
    "                length_penalty=1.0,  # Balanced length\n",
    "                no_repeat_ngram_size=2  # Avoid repeating bigrams\n",
    "            )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean and validate JSON\n",
    "            json_start = response.find('{')\n",
    "            json_end = response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = response[json_start:json_end]\n",
    "                # Parse and validate structure\n",
    "                data = json.loads(json_str)\n",
    "                if isinstance(data, dict) and \"queries\" in data and len(data[\"queries\"]) == 4:\n",
    "                    return json_str\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(f\"Failed to generate valid response after {max_attempts} attempts: {str(e)}\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "    # Fallback to template-based variations\n",
    "    return json.dumps({\n",
    "        \"queries\": [\n",
    "            input_query,\n",
    "            f\"Define {input_query.lower().replace('what is ', '')}\",\n",
    "            f\"Explain the concept of {input_query.lower().replace('what is ', '')}\",\n",
    "            f\"Describe {input_query.lower().replace('what is ', '')}\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def expand_query(query: str) -> List[str]:\n",
    "    \"\"\"Generate 4 semantically similar variations of the input query.\"\"\"\n",
    "    \n",
    "    # Format prompt with escaped braces for JSON template\n",
    "    prompt = INSTRUCTION_TEMPLATE.format(query=query)\n",
    "    \n",
    "    try:\n",
    "        # Generate response\n",
    "        response_text = generate_response(prompt, query)  # Pass query to fallback\n",
    "        \n",
    "        # Extract JSON\n",
    "        json_data = json.loads(response_text)\n",
    "        if not json_data:\n",
    "            raise ValueError(\"Could not extract JSON from response\")\n",
    "            \n",
    "        # Validate with Pydantic\n",
    "        validated = QueryExpansion(**json_data)\n",
    "        return validated.queries\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating variations: {e}\")\n",
    "        # Fallback variations\n",
    "        return [\n",
    "            query,\n",
    "            f\"Define {query.lower().replace('what is ', '')}\",\n",
    "            f\"Explain the concept of {query.lower().replace('what is ', '')}\",\n",
    "            f\"Describe {query.lower().replace('what is ', '')}\"\n",
    "        ]\n",
    "\n",
    "# Test the expansion\n",
    "test_query = \"What is psychology?\"\n",
    "print(\"\\nOriginal query:\", test_query)\n",
    "\n",
    "expanded_queries = expand_query(test_query)\n",
    "print(\"\\nExpanded queries:\")\n",
    "for i, q in enumerate(expanded_queries, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "# Save expanded queries for next step\n",
    "print(\"\\nSaving expanded queries...\")\n",
    "with open(\"artifacts/expanded_queries.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"original_query\": test_query,\n",
    "        \"expanded_queries\": expanded_queries\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339b9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading FAISS index...\n",
      "\n",
      "Loading expanded queries...\n",
      "\n",
      "Original query: What is psychology?\n",
      "Number of variations: 4\n",
      "\n",
      "Processing queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 4/4 [00:00<00:00, 40.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "\n",
      "Sample results for first query:\n",
      "\n",
      "Query: What is psychology?\n",
      "Top 3 chunks:\n",
      "\n",
      "1. Score: 0.736\n",
      "   Page: 43 (logical: 31)\n",
      "   Section: \n",
      "   Text: of the most influential schools of thought within psychology’s history was behaviorism. Behaviorism focused on making psychology an objective science by studying overt behavior and deemphasizing the i...\n",
      "\n",
      "2. Score: 0.731\n",
      "   Page: 42 (logical: 30)\n",
      "   Section: \n",
      "   Text: Key Terms American Psychological Association (APA) professional organization representing psychologists in the United States behaviorism focus on observing and controlling behavior biopsychology study...\n",
      "\n",
      "3. Score: 0.705\n",
      "   Page: 43 (logical: 31)\n",
      "   Section: \n",
      "   Text: made up of several major subdivisions with unique perspectives. Biological psychology involves the study of the biological bases of behavior. Sensation and perception refer to the area of psychology t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: COSINE SIMILARITY RERANKING\n",
    "\"\"\"\n",
    "In this step, we:\n",
    "1. Get initial candidates (5 per query) using FAISS\n",
    "2. Rerank using cosine similarity\n",
    "3. Select top 3 chunks per query\n",
    "4. Structure results with metadata\n",
    "\n",
    "Why cosine similarity?\n",
    "- Fast and efficient\n",
    "- Works well with normalized embeddings\n",
    "- No additional models needed\n",
    "- Purely CPU-based computation\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Third-party imports\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Reload FAISS index\n",
    "print(\"Reloading FAISS index...\")\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"artifacts/faiss_index\",\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Load saved queries\n",
    "print(\"\\nLoading expanded queries...\")\n",
    "with open(\"artifacts/expanded_queries.pkl\", \"rb\") as f:\n",
    "    query_data = pickle.load(f)\n",
    "    original_query = query_data[\"original_query\"]\n",
    "    expanded_queries = query_data[\"expanded_queries\"]\n",
    "\n",
    "print(f\"\\nOriginal query: {original_query}\")\n",
    "print(f\"Number of variations: {len(expanded_queries)}\")\n",
    "\n",
    "def get_initial_chunks(query: str, k: int = 5) -> List[Document]:\n",
    "    \"\"\"Get initial chunks using FAISS similarity search.\"\"\"\n",
    "    return vectorstore.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "def process_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single query and return top chunks with metadata.\"\"\"\n",
    "    # Get initial chunks with scores\n",
    "    chunks_and_scores = get_initial_chunks(query, k=5)\n",
    "    \n",
    "    # Sort by score (cosine similarity)\n",
    "    sorted_chunks = sorted(chunks_and_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top 3\n",
    "    top_chunks = sorted_chunks[:3]\n",
    "    \n",
    "    # Format results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"chunks\": [\n",
    "            {\n",
    "                \"score\": float(score),  # Already cosine similarity\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            for doc, score in top_chunks\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Process all queries\n",
    "print(\"\\nProcessing queries...\")\n",
    "all_results = {\n",
    "    \"original_query\": original_query,\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "for query in tqdm(expanded_queries, desc=\"Processing queries\"):\n",
    "    result = process_query(query)\n",
    "    all_results[\"results\"].append(result)\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "with open(\"artifacts/ranked_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample results for first query:\")\n",
    "first_result = all_results[\"results\"][0]\n",
    "print(f\"\\nQuery: {first_result['query']}\")\n",
    "print(f\"Top 3 chunks:\")\n",
    "for i, chunk in enumerate(first_result[\"chunks\"], 1):\n",
    "    print(f\"\\n{i}. Score: {chunk['score']:.3f}\")\n",
    "    print(f\"   Page: {chunk['metadata']['physical_page']} (logical: {chunk['metadata']['logical_page']})\")\n",
    "    print(f\"   Section: {chunk['metadata']['section']}\")\n",
    "    print(f\"   Text: {chunk['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f04273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ranked results...\n",
      "\n",
      "Loading T5-small model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing results...\n",
      "\n",
      "Generating final answer...\n",
      "\n",
      "Saving complete response...\n",
      "\n",
      "Final Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "Psychology is a diverse discipline that is made up of several major subdivisions with unique perspectives. Biological psychology involves the study of the biological bases of behavior. Sensation and perception refer to the area of psychology that is focused on how information from our sensory modalities is [Page 43 - ]:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Full results including metadata saved to artifacts/final_response.json\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: FINAL ANSWER GENERATION\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# Load ranked results\n",
    "print(\"Loading ranked results...\")\n",
    "with open(\"artifacts/ranked_results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "original_query = all_results[\"original_query\"]\n",
    "query_results = all_results[\"results\"]\n",
    "\n",
    "def format_context_with_metadata(results):\n",
    "    \"\"\"Format retrieved chunks with full metadata.\"\"\"\n",
    "    formatted_results = []\n",
    "    \n",
    "    # Process each query variation\n",
    "    for query_idx, query_result in enumerate(results):\n",
    "        query_info = {\n",
    "            \"query\": query_result[\"query\"],\n",
    "            \"chunks\": []\n",
    "        }\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in query_result[\"chunks\"][:2]:  # Top 2 chunks per query\n",
    "            chunk_info = {\n",
    "                \"content\": chunk[\"content\"].strip(),\n",
    "                \"metadata\": {\n",
    "                    \"physical_page\": chunk[\"metadata\"][\"physical_page\"],\n",
    "                    \"logical_page\": chunk[\"metadata\"][\"logical_page\"],\n",
    "                    \"section\": chunk[\"metadata\"][\"section\"]\n",
    "                },\n",
    "                \"score\": chunk[\"score\"]\n",
    "            }\n",
    "            query_info[\"chunks\"].append(chunk_info)\n",
    "        \n",
    "        formatted_results.append(query_info)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def format_display_context(formatted_results):\n",
    "    \"\"\"Format context for the model prompt.\"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for query_result in formatted_results:\n",
    "        for chunk in query_result[\"chunks\"]:\n",
    "            content = chunk[\"content\"]\n",
    "            page = chunk[\"metadata\"][\"physical_page\"]\n",
    "            section = chunk[\"metadata\"][\"section\"]\n",
    "            \n",
    "            # Format with detailed citation\n",
    "            context_parts.append(\n",
    "                f\"[Page {page} - {section}]:\\n{content}\"\n",
    "            )\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Improved prompt template\n",
    "ANSWER_TEMPLATE = \"\"\"Answer the question using only the provided context. Include page citations [Page X] to support your answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer directly and clearly\n",
    "2. Use information only from the context\n",
    "3. Include page citations [Page X]\n",
    "4. Be concise but complete\n",
    "5. Focus on explaining concepts, not just listing terms\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\nLoading T5-small model...\")\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Generate answer using T5-small with optimized parameters.\"\"\"\n",
    "    try:\n",
    "        # Prepare input\n",
    "        prompt = ANSWER_TEMPLATE.format(\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "        \n",
    "        # Tokenize with length limits\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Generate with improved parameters\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=200,\n",
    "            min_length=50,\n",
    "            num_beams=3,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=1.0\n",
    "        )\n",
    "        \n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Post-process to ensure citations are preserved\n",
    "        if \"[Page\" not in answer:\n",
    "            answer += \"\\n\\nNote: This answer is based on information from the provided context.\"\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generation: {e}\")\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Process results with metadata\n",
    "print(\"\\nProcessing results...\")\n",
    "formatted_results = format_context_with_metadata(query_results)\n",
    "display_context = format_display_context(formatted_results)\n",
    "\n",
    "# Generate final answer\n",
    "print(\"\\nGenerating final answer...\")\n",
    "final_answer = generate_answer(original_query, display_context)\n",
    "\n",
    "# Save complete response with full metadata\n",
    "print(\"\\nSaving complete response...\")\n",
    "complete_response = {\n",
    "    \"question\": original_query,\n",
    "    \"queries_and_results\": formatted_results,\n",
    "    \"generated_answer\": final_answer,\n",
    "    \"model_used\": model_name,\n",
    "    \"generation_parameters\": {\n",
    "        \"max_length\": 200,\n",
    "        \"min_length\": 50,\n",
    "        \"num_beams\": 3,\n",
    "        \"temperature\": 0.7,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"early_stopping\": True,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"length_penalty\": 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"artifacts/final_response.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complete_response, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(\"-\" * 80)\n",
    "print(final_answer)\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nFull results including metadata saved to artifacts/final_response.json\")\n",
    "\n",
    "# Cleanup\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
