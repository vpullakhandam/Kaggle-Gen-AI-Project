{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f7c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation and Imports\n",
    "!pip -q install langchain langchain-community langchain-text-splitters\n",
    "!pip -q install pypdf pymupdf sentence-transformers\n",
    "!pip -q install numpy pandas tqdm faiss-cpu\n",
    "!pip -q install instructor pydantic transformers accelerate torch\n",
    "\n",
    "# Imports\n",
    "from pathlib import Path\n",
    "import json, re, pickle\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LLM imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from instructor import patch\n",
    "\n",
    "# Pydantic for validation\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# Optional: keep CPU thread usage sane in notebooks\n",
    "if not torch.cuda.is_available():\n",
    "    torch.set_num_threads(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57267dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m BOOK_PDF_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Sources/book.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m SECTIONS_JSON_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./working/page_to_section.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Verify files exist\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BOOK_PDF_PATH = Path(\"../Sources/book.pdf\")\n",
    "SECTIONS_JSON_PATH = Path(\"./working/page_to_section.json\")\n",
    "\n",
    "# Verify files exist\n",
    "assert BOOK_PDF_PATH.exists(), f\"PDF not found: {BOOK_PDF_PATH}\"\n",
    "assert SECTIONS_JSON_PATH.exists(), f\"Sections JSON not found: {SECTIONS_JSON_PATH}\"\n",
    "\n",
    "# Load sections mapping\n",
    "print(\"Loading sections mapping...\")\n",
    "with open(SECTIONS_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    sections_map = json.load(f)\n",
    "\n",
    "# Ensure all keys are strings\n",
    "sections_map = {str(k): v for k, v in sections_map.items()}\n",
    "print(f\"Loaded {len(sections_map)} page-to-section mappings\")\n",
    "def clean_page_text(text: str) -> str:\n",
    "    \"\"\"Light cleaning for a single page string.\"\"\"\n",
    "    if not text: \n",
    "        return \"\"\n",
    "    \n",
    "    # Remove common boilerplate text\n",
    "    text = re.sub(r'Access for free at openstax\\.org\\.*', '', text)\n",
    "    text = re.sub(r'LINK TO LEARNING.*?(?=\\n|$)', '', text)  # Remove LINK TO LEARNING sections\n",
    "    text = re.sub(r'Watch a brief video.*?(?=\\n|$)', '', text)  # Remove video references\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # Remove URLs\n",
    "    \n",
    "    # Basic text normalization\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"-\\n(\\w)\", r\"\\1\", text)       # de-hyphenate linebreaks\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)       # strip trailing spaces before newline\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)       # collapse multi-spaces\n",
    "    \n",
    "    # Remove multiple consecutive newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def split_into_word_windows(text: str, size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Split ONE page into overlapping word windows. Returns list of strings.\"\"\"\n",
    "    words = text.replace(\"\\n\", \" \").split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(words):\n",
    "        # Take size words for this chunk\n",
    "        chunk = words[start_idx:start_idx + size]\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        \n",
    "        # Move start_idx forward by (size - overlap) to create overlap\n",
    "        start_idx += (size - overlap)\n",
    "        \n",
    "        # If we can't make a full chunk anymore, break\n",
    "        if start_idx + size > len(words):\n",
    "            # Add final chunk if there are remaining words\n",
    "            remaining = words[start_idx:]\n",
    "            if remaining:\n",
    "                chunks.append(\" \".join(remaining))\n",
    "            break\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(physical_page: int, idx: int) -> str:\n",
    "    \"\"\"Make a stable chunk id like 'p19_w003' that preserves page number.\"\"\"\n",
    "    return f\"p{physical_page}_w{idx:03d}\"\n",
    "\n",
    "def process_pdf(pdf_path: str, \n",
    "               sections_map: dict,\n",
    "               start_page: int = 19,    # Skip front matter\n",
    "               end_page: int = 638,     # Stop before references\n",
    "               logical_offset: int = 12,\n",
    "               chunk_size: int = 220,    \n",
    "               chunk_overlap: int = 40    \n",
    "               ) -> List[Document]:\n",
    "    \"\"\"Process PDF while maintaining page boundaries and creating overlapping chunks.\"\"\"\n",
    "    \n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    docs_all = loader.load()\n",
    "    \n",
    "    # Filter pages between start_page and end_page\n",
    "    page_docs = [d for d in docs_all if start_page <= int(d.metadata.get(\"page\", 0)) + 1 <= end_page]\n",
    "    chunks = []\n",
    "    \n",
    "    for d in tqdm(page_docs, desc=\"Processing pages\"):\n",
    "        physical_page = int(d.metadata.get(\"page\", 0)) + 1\n",
    "        logical_page = physical_page - logical_offset\n",
    "        section = sections_map.get(str(logical_page), \"\")\n",
    "        \n",
    "        page_text = clean_page_text(d.page_content)\n",
    "        windows = split_into_word_windows(\n",
    "            text=page_text,\n",
    "            size=chunk_size,\n",
    "            overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        for i, window in enumerate(windows):\n",
    "            metadata = {\n",
    "                \"chunk_id\": make_chunk_id(physical_page, i),\n",
    "                \"physical_page\": physical_page,\n",
    "                \"logical_page\": logical_page,\n",
    "                \"section\": section,\n",
    "                \"source\": str(pdf_path),\n",
    "                \"chunk_size_words\": chunk_size,\n",
    "                \"chunk_overlap_words\": chunk_overlap,\n",
    "            }\n",
    "            chunks.append(Document(page_content=window, metadata=metadata))\n",
    "            \n",
    "    return chunks\n",
    "\n",
    "# Usage\n",
    "chunks = process_pdf(\n",
    "    pdf_path=BOOK_PDF_PATH,\n",
    "    sections_map=sections_map,\n",
    "    start_page=19,   # Skip front matter\n",
    "    end_page=638    # Stop before references\n",
    ")\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "for c in chunks[:5]:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"id     : {c.metadata['chunk_id']}\")\n",
    "    print(f\"page   : phys {c.metadata['physical_page']} | logical {c.metadata['logical_page']}\")\n",
    "    print(f\"section: {c.metadata['section'] or '(none)'}\")\n",
    "    print(f\"text   : {c.page_content}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9a1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/ns3sdnhj22n2q3hsq7mpbgv40000gn/T/ipykernel_1528/2111709763.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "\n",
      "Building FAISS index from chunks...\n",
      "\n",
      "Saving index to artifacts/faiss_index...\n",
      "\n",
      "Verifying saved index...\n",
      "\n",
      "Test Results:\n",
      "Query: What is psychology?\n",
      "\n",
      "Result 1:\n",
      "Page: 20 (logical: 8)\n",
      "Section: introduction_to_psychology/what_is_psychology\n",
      "Text: understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. The scientific method is also a form of empiricism. An empirical method for acqu...\n",
      "\n",
      "Result 2:\n",
      "Page: 20 (logical: 8)\n",
      "Section: introduction_to_psychology/what_is_psychology\n",
      "Text: 2002). Nash was the subject of the 2001 movie A Beautiful Mind. Why did these people have these experiences? How does the human brain work? And what is the connection between the brain’s internal proc...\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: EMBEDDINGS AND VECTOR STORE\n",
    "\"\"\"\n",
    "In this step, we create a semantic search index for our text chunks:\n",
    "\n",
    "1. Create embeddings using HuggingFace's sentence-transformers\n",
    "   - Using all-MiniLM-L6-v2: Efficient model that works well on CPU\n",
    "   - Normalized embeddings for better cosine similarity\n",
    "\n",
    "2. Build FAISS index for fast similarity search\n",
    "   - FAISS is efficient for large-scale similarity search\n",
    "   - Works well on CPU for our dataset size\n",
    "\n",
    "3. Save index for reuse\n",
    "   - Persists both the FAISS index and metadata\n",
    "   - Allows quick reloading without recomputing embeddings\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Efficient CPU model\n",
    "INDEX_SAVE_PATH = Path(\"./artifacts/faiss_index\")                # Where to save the index\n",
    "INDEX_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},           # Force CPU - works well for this model\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # Better for cosine similarity\n",
    ")\n",
    "\n",
    "# Create FAISS index from our chunks\n",
    "print(\"\\nBuilding FAISS index from chunks...\")\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,           # Our preprocessed text chunks\n",
    "    embedding=embeddings       # The embedding model\n",
    ")\n",
    "\n",
    "# Save index for reuse\n",
    "print(f\"\\nSaving index to {INDEX_SAVE_PATH}...\")\n",
    "vectorstore.save_local(str(INDEX_SAVE_PATH))\n",
    "\n",
    "# Quick verification of saved index\n",
    "print(\"\\nVerifying saved index...\")\n",
    "reloaded_vectorstore = FAISS.load_local(\n",
    "    folder_path=str(INDEX_SAVE_PATH),\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required in notebooks\n",
    ")\n",
    "\n",
    "# Test a simple query to verify everything works\n",
    "test_query = \"What is psychology?\"\n",
    "test_results = reloaded_vectorstore.similarity_search(\n",
    "    query=test_query,\n",
    "    k=2  # Get top 2 results\n",
    ")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Query: {test_query}\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Page: {doc.metadata['physical_page']} (logical: {doc.metadata['logical_page']})\")\n",
    "    print(f\"Section: {doc.metadata['section']}\")\n",
    "    print(f\"Text: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45d8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page to section mapping from working/page_to_section.json...\n",
      "Loaded mapping for 494 valid physical pages\n",
      "Loading queries from ../Sources/queries.json...\n",
      "Loaded 50 queries\n",
      "\n",
      "Reloading FAISS index...\n",
      "\n",
      "Loading FLAN-T5-small with optimized settings...\n",
      "\n",
      "Processing all queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries:  24%|██▍       | 12/50 [00:00<00:00, 112.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed query 10\n",
      "Processed query 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries:  50%|█████     | 25/50 [00:00<00:00, 119.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed query 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 50/50 [00:00<00:00, 116.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed query 40\n",
      "Processed query 50\n",
      "\n",
      "Creating submission.csv with 50 results...\n",
      "Results saved to: submission.csv\n",
      "CSV shape: (50, 4)\n",
      "\n",
      "First few rows:\n",
      "  ID                                            context  \\\n",
      "0  1  understanding of the mind is so limited, since...   \n",
      "1  2  the other hand, serve as interconnected inform...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Psychology refers to the scientific study of t...   \n",
      "1  2 Cells of the Nervous System LEARNING OBJECTI...   \n",
      "\n",
      "                                          references  \n",
      "0  {\"sections\": [\"introduction_to_psychology/what...  \n",
      "1  {\"sections\": [\"biopsychology/cells_of_the_nerv...  \n",
      "\n",
      "Sample result:\n",
      "ID: 1\n",
      "Context: understanding of the mind is so limited, since thoughts, at least as we experience them, are neither...\n",
      "Answer: Psychology refers to the scientific study of the mind and behavior. The scientific method is also a ...\n",
      "References: {\"sections\": [\"introduction_to_psychology/what_is_psychology\"], \"pages\": [\"20\"]}\n",
      "\n",
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: PROCESS ALL QUERIES AND GENERATE SUBMISSION.CSV\n",
    "\"\"\"\n",
    "Process all queries from queries.json and save results to submission.csv with proper format:\n",
    "- ID: Query identifier\n",
    "- context: Retrieved context from textbook (top 3 chunks combined)\n",
    "- answer: Generated answer based on the context\n",
    "- references: JSON with sections and pages\n",
    "\n",
    "Filter chunks to only include pages mentioned in page_to_section.json (with offset of 12)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load page to section mapping\n",
    "page_to_section_path = Path(\"./working/page_to_section.json\")\n",
    "print(f\"Loading page to section mapping from {page_to_section_path}...\")\n",
    "\n",
    "with open(page_to_section_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    page_to_section = json.load(f)\n",
    "\n",
    "# Convert to physical page numbers (add offset of 12)\n",
    "valid_physical_pages = set()\n",
    "for logical_page_str, section in page_to_section.items():\n",
    "    logical_page = int(logical_page_str)\n",
    "    physical_page = logical_page + 12  # Add offset\n",
    "    valid_physical_pages.add(physical_page)\n",
    "\n",
    "print(f\"Loaded mapping for {len(valid_physical_pages)} valid physical pages\")\n",
    "\n",
    "# Load queries from queries.json\n",
    "queries_path = Path(\"../Sources/queries.json\")\n",
    "print(f\"Loading queries from {queries_path}...\")\n",
    "\n",
    "with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(queries_data)} queries\")\n",
    "\n",
    "# Reload FAISS index and model\n",
    "print(\"\\nReloading FAISS index...\")\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"artifacts/faiss_index\",\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Load lightweight model for answer generation  \n",
    "print(\"\\nLoading FLAN-T5-small with optimized settings...\")\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Use float32 for stability\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Free up memory\n",
    "import gc\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "def get_filtered_chunks_for_query(query: str, k: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get top k chunks for a single query with hybrid reranking like the competition winner.\"\"\"\n",
    "    # Get more chunks initially for reranking\n",
    "    chunks_and_scores = vectorstore.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=k * 3  # Get more candidates for reranking\n",
    "    )\n",
    "    \n",
    "    # Filter to only include valid pages and format results\n",
    "    filtered_chunks = []\n",
    "    for doc, score in chunks_and_scores:\n",
    "        physical_page = doc.metadata[\"physical_page\"]\n",
    "        \n",
    "        # Only include chunks from valid pages\n",
    "        if physical_page in valid_physical_pages:\n",
    "            # Get section from page_to_section mapping\n",
    "            logical_page = physical_page - 12  # Remove offset\n",
    "            section = page_to_section.get(str(logical_page), \"\")\n",
    "            \n",
    "            chunk_info = {\n",
    "                \"content\": doc.page_content.strip(),\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": {\n",
    "                    \"physical_page\": physical_page,\n",
    "                    \"logical_page\": logical_page,\n",
    "                    \"section\": section,\n",
    "                    \"chunk_id\": doc.metadata.get(\"chunk_id\", \"\")\n",
    "                }\n",
    "            }\n",
    "            filtered_chunks.append(chunk_info)\n",
    "    \n",
    "    # Apply hybrid reranking (like the competition winner)\n",
    "    reranked_chunks = rerank_chunks_hybrid(filtered_chunks, query)\n",
    "    \n",
    "    return reranked_chunks[:k]  # Return top k reranked chunks\n",
    "\n",
    "def rerank_chunks_hybrid(chunks: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rerank chunks using hybrid approach: semantic similarity + keyword overlap\n",
    "    (Similar to the competition winner's strategy)\n",
    "    \"\"\"\n",
    "    query_terms = set(query.lower().split())\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Calculate keyword overlap\n",
    "        content_terms = set(chunk[\"content\"].lower().split())\n",
    "        keyword_overlap = len(query_terms & content_terms)\n",
    "        \n",
    "        # Combine semantic score with keyword overlap\n",
    "        semantic_score = 1.0 - chunk[\"score\"]  # Convert distance to similarity\n",
    "        keyword_score = keyword_overlap / max(len(query_terms), 1)\n",
    "        \n",
    "        # Weighted combination (60% semantic, 40% keyword)\n",
    "        chunk[\"hybrid_score\"] = (0.6 * semantic_score) + (0.4 * keyword_score)\n",
    "        chunk[\"keyword_overlap\"] = keyword_overlap\n",
    "    \n",
    "    # Sort by hybrid score (higher is better)\n",
    "    chunks.sort(key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def format_context(chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Format chunks into context string without page numbers.\"\"\"\n",
    "    context_parts = []\n",
    "    for chunk in chunks:\n",
    "        content = chunk[\"content\"]\n",
    "        context_parts.append(content)\n",
    "    \n",
    "    return \" \".join(context_parts)\n",
    "\n",
    "def format_references(chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Format references as JSON string with sections and pages.\"\"\"\n",
    "    # Extract unique sections and pages\n",
    "    sections = []\n",
    "    pages = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        section = chunk[\"metadata\"][\"section\"]\n",
    "        page = str(chunk[\"metadata\"][\"physical_page\"])\n",
    "        \n",
    "        if section and section not in sections:\n",
    "            sections.append(section)\n",
    "        if page not in pages:\n",
    "            pages.append(page)\n",
    "    \n",
    "    # Sort for consistency\n",
    "    sections.sort()\n",
    "    pages.sort(key=int)  # Sort numerically\n",
    "    \n",
    "    references = {\n",
    "        \"sections\": sections,\n",
    "        \"pages\": pages\n",
    "    }\n",
    "    \n",
    "    return json.dumps(references)\n",
    "\n",
    "# Simplified approach - no complex template needed\n",
    "def extract_answer_from_context(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract answer using improved strategy similar to competition winner.\n",
    "    Combines semantic similarity with better sentence selection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean and split context into sentences\n",
    "        sentences = []\n",
    "        for sent in re.split(r'[.!?]+', context):\n",
    "            sent = sent.strip()\n",
    "            if len(sent) > 15:  # Filter out short fragments\n",
    "                sentences.append(sent)\n",
    "        \n",
    "        if not sentences:\n",
    "            return \"No relevant information found in the context.\"\n",
    "        \n",
    "        # Extract question keywords (improved stopword filtering)\n",
    "        question_lower = question.lower()\n",
    "        stop_words = {\n",
    "            'what', 'how', 'why', 'when', 'where', 'which', 'who', 'is', 'are', 'was', 'were', \n",
    "            'do', 'does', 'did', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', \n",
    "            'for', 'of', 'with', 'by', 'that', 'this', 'can', 'you', 'be', 'it', 'as', 'from'\n",
    "        }\n",
    "        question_words = [word for word in re.findall(r'\\b\\w+\\b', question_lower) \n",
    "                         if word not in stop_words and len(word) > 2]\n",
    "        \n",
    "        # Score sentences using improved methodology\n",
    "        sentence_scores = []\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            sentence_lower = sentence.lower()\n",
    "            sentence_words = set(sentence_lower.split())\n",
    "            \n",
    "            # Keyword overlap score (similar to competition winner)\n",
    "            keyword_overlap = len(set(question_words) & sentence_words)\n",
    "            score += keyword_overlap * 2.0\n",
    "            \n",
    "            # Semantic similarity using sentence embeddings\n",
    "            try:\n",
    "                q_embedding = sentence_model.encode([question])\n",
    "                s_embedding = sentence_model.encode([sentence])\n",
    "                similarity = np.dot(q_embedding[0], s_embedding[0]) / (\n",
    "                    np.linalg.norm(q_embedding[0]) * np.linalg.norm(s_embedding[0])\n",
    "                )\n",
    "                score += similarity * 3.0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Boost definitional and explanatory sentences\n",
    "            definition_phrases = [\n",
    "                'is defined as', 'refers to', 'means', 'is the', 'involves', 'includes',\n",
    "                'is a', 'are', 'describe', 'explain', 'definition', 'concept', 'theory'\n",
    "            ]\n",
    "            if any(phrase in sentence_lower for phrase in definition_phrases):\n",
    "                score += 1.5\n",
    "            \n",
    "            # Prefer sentences with more content\n",
    "            if len(sentence.split()) > 10:\n",
    "                score += 0.5\n",
    "                \n",
    "            sentence_scores.append((score, sentence, i, keyword_overlap))\n",
    "        \n",
    "        # Sort by score (higher is better)\n",
    "        sentence_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Build comprehensive answer (like competition winner)\n",
    "        answer_parts = []\n",
    "        used_sentences = set()\n",
    "        total_length = 0\n",
    "        \n",
    "        # Take top scoring sentences that add value\n",
    "        for score, sentence, idx, overlap in sentence_scores:\n",
    "            if (score > 0.8 and idx not in used_sentences and \n",
    "                total_length < 350 and len(answer_parts) < 3):\n",
    "                \n",
    "                clean_sentence = sentence.strip()\n",
    "                if clean_sentence and not clean_sentence.endswith('.'):\n",
    "                    clean_sentence += '.'\n",
    "                \n",
    "                answer_parts.append(clean_sentence)\n",
    "                used_sentences.add(idx)\n",
    "                total_length += len(clean_sentence)\n",
    "        \n",
    "        if answer_parts:\n",
    "            answer = ' '.join(answer_parts)\n",
    "            # Ensure reasonable length\n",
    "            if len(answer) > 450:\n",
    "                answer = answer[:447] + '...'\n",
    "            return answer\n",
    "        else:\n",
    "            # Fallback to best single sentence\n",
    "            if sentence_scores:\n",
    "                best_sentence = sentence_scores[0][1].strip()\n",
    "                if not best_sentence.endswith('.'):\n",
    "                    best_sentence += '.'\n",
    "                return best_sentence\n",
    "            \n",
    "            # Last resort\n",
    "            return sentences[0].strip() + '.'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extractive QA: {str(e)}\")\n",
    "        # Simple fallback\n",
    "        sentences = context.split('.')[:2]\n",
    "        return '. '.join(s.strip() for s in sentences if s.strip()) + '.'\n",
    "\n",
    "# Alias for backward compatibility\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Wrapper for extractive answer generation.\"\"\"\n",
    "    return extract_answer_from_context(question, context)\n",
    "\n",
    "# Process all queries\n",
    "print(\"\\nProcessing all queries...\")\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "for query_item in tqdm(queries_data, desc=\"Processing queries\"):\n",
    "    query_id = query_item[\"query_id\"]\n",
    "    question = query_item[\"question\"]\n",
    "    \n",
    "    try:\n",
    "        # Get top 3 filtered chunks for this query\n",
    "        top_chunks = get_filtered_chunks_for_query(question, k=3)\n",
    "        \n",
    "        if not top_chunks:\n",
    "            print(f\"Warning: No valid chunks found for query {query_id}\")\n",
    "            # Fallback - get any chunks\n",
    "            chunks_and_scores = vectorstore.similarity_search_with_score(query=question, k=3)\n",
    "            top_chunks = []\n",
    "            for doc, score in chunks_and_scores:\n",
    "                top_chunks.append({\n",
    "                    \"content\": doc.page_content.strip(),\n",
    "                    \"score\": float(score),\n",
    "                    \"metadata\": {\n",
    "                        \"physical_page\": doc.metadata[\"physical_page\"],\n",
    "                        \"logical_page\": doc.metadata.get(\"logical_page\", 0),\n",
    "                        \"section\": \"\",\n",
    "                        \"chunk_id\": doc.metadata.get(\"chunk_id\", \"\")\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Format context (without page numbers in the text)\n",
    "        context = format_context(top_chunks)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = generate_answer(question, context)\n",
    "        \n",
    "        # Format references as JSON\n",
    "        references = format_references(top_chunks)\n",
    "        \n",
    "        # Add to submission data\n",
    "        submission_data.append({\n",
    "            \"ID\": query_id,\n",
    "            \"context\": context,\n",
    "            \"answer\": answer,\n",
    "            \"references\": references\n",
    "        })\n",
    "        \n",
    "        # Progress update\n",
    "        if int(query_id) % 10 == 0:\n",
    "            print(f\"Processed query {query_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {query_id}: {str(e)}\")\n",
    "        # Add error entry\n",
    "        submission_data.append({\n",
    "            \"ID\": query_id,\n",
    "            \"context\": \"Error retrieving context\",\n",
    "            \"answer\": f\"Error: {str(e)}\",\n",
    "            \"references\": json.dumps({\"sections\": [], \"pages\": []})\n",
    "        })\n",
    "\n",
    "# Create submission CSV\n",
    "print(f\"\\nCreating submission.csv with {len(submission_data)} results...\")\n",
    "df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = Path(\"submission.csv\")\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Results saved to: {csv_path}\")\n",
    "print(f\"CSV shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Display sample of results\n",
    "print(f\"\\nSample result:\")\n",
    "sample = df.iloc[0]\n",
    "print(f\"ID: {sample['ID']}\")\n",
    "print(f\"Context: {sample['context'][:100]}...\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")\n",
    "print(f\"References: {sample['references']}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
